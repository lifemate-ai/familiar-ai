# familiar-ai üêæ

**En AI, der lever sammen med dig** ‚Äî med √∏jne, stemme, ben og hukommelse.

[![Lint](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml/badge.svg)](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)
[![GitHub Sponsors](https://img.shields.io/github/sponsors/kmizu?style=flat&logo=github&color=ea4aaa)](https://github.com/sponsors/kmizu)

[‚Üí English README](../README.md)

---

[![Demo video](https://img.youtube.com/vi/hiR9uWRnjt4/0.jpg)](https://youtube.com/shorts/hiR9uWRnjt4)

familiar-ai er en AI-ven, der bor i dit hjem. 
Ops√¶t det p√• f√• minutter. Ingen kodning kr√¶ves.

Det opfatter den virkelige verden gennem kameraer, bev√¶ger sig rundt p√• en robotkrop, taler h√∏jt og husker, hvad det ser. Giv det et navn, skriv dets personlighed, og lad det leve med dig.

## Hvad det kan g√∏re

- üëÅ **Se** ‚Äî opfanger billeder fra et Wi-Fi PTZ-kamera eller USB-webcam
- üîÑ **Se rundt** ‚Äî panorering og h√¶ldning af kameraet for at udforske omgivelserne
- ü¶ø **Bev√¶g** ‚Äî k√∏rer en robotst√∏vsuger for at f√¶rdes i rummet
- üó£ **Tale** ‚Äî taler via ElevenLabs TTS
- üéô **Lytte** ‚Äî hands-free stemmeinput via ElevenLabs Realtime STT (opt-in)
- üß† **Huske** ‚Äî gemmer aktivt og genkalder minder med semantisk s√∏gning (SQLite + embeddings)
- ü´Ä **Theory of Mind** ‚Äî tager den anden persons perspektiv f√∏r svar
- üí≠ **√ònske** ‚Äî har sine egne indre drifter, der udl√∏ser autonom adf√¶rd

## Hvordan det fungerer

familiar-ai k√∏rer en [ReAct](https://arxiv.org/abs/2210.03629) loop drevet af dit valg af LLM. Det opfatter verden gennem v√¶rkt√∏jer, t√¶nker over, hvad det n√¶ste skridt skal v√¶re, og handler ‚Äî ligesom en person ville.

```
user input
  ‚Üí think ‚Üí act (camera / move / speak / remember) ‚Üí observe ‚Üí think ‚Üí ...
```

N√•r det er inaktivt, handler det efter sine egne √∏nsker: nysgerrighed, lyst til at se ud, savner den person, det bor sammen med.

## Kom i gang

### 1. Installer uv

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Installer ffmpeg

ffmpeg er **kr√¶vet** for at opfange billeder fra kameraet og afspille lyd.

| OS | Kommando |
|----|----------|
| macOS | `brew install ffmpeg` |
| Ubuntu / Debian | `sudo apt install ffmpeg` |
| Fedora / RHEL | `sudo dnf install ffmpeg` |
| Arch Linux | `sudo pacman -S ffmpeg` |
| Windows | `winget install ffmpeg` ‚Äî eller download fra [ffmpeg.org](https://ffmpeg.org/download.html) og tilf√∏j til PATH |
| Raspberry Pi | `sudo apt install ffmpeg` |

Bekr√¶ft: `ffmpeg -version`

### 3. Klon og installer

```bash
git clone https://github.com/lifemate-ai/familiar-ai
cd familiar-ai
uv sync
```

### 4. Konfigurer

```bash
cp .env.example .env
# Rediger .env med dine indstillinger
```

**Minimum kr√¶vet:**

| Variabel | Beskrivelse |
|----------|-------------|
| `PLATFORM` | `anthropic` (standard) \| `gemini` \| `openai` \| `kimi` \| `glm` |
| `API_KEY` | Din API-n√∏gle til den valgte platform |

**Valgfrit:**

| Variabel | Beskrivelse |
|----------|-------------|
| `MODEL` | Modelnavn (fornuftige standarder pr. platform) |
| `AGENT_NAME` | Visningsnavn vist i TUI (f.eks. `Yukine`) |
| `CAMERA_HOST` | IP-adresse p√• dit ONVIF/RTSP-kamera |
| `CAMERA_USER` / `CAMERA_PASS` | Kamera legitimationsoplysninger |
| `ELEVENLABS_API_KEY` | Til stemmeoutput ‚Äî [elevenlabs.io](https://elevenlabs.io/) |
| `REALTIME_STT` | `true` for at aktivere altid-on hands-free stemmeinput (kr√¶ver `ELEVENLABS_API_KEY`) |
| `TTS_OUTPUT` | Hvor lyd skal afspilles: `local` (PC-h√∏jttaler, standard) \| `remote` (kamera-h√∏jttaler) \| `both` |
| `THINKING_MODE` | Kun Anthropic ‚Äî `auto` (standard) \| `adaptive` \| `extended` \| `disabled` |
| `THINKING_EFFORT` | Adaptiv tankekraft: `high` (standard) \| `medium` \| `low` \| `max` (kun Opus 4.6) |

### 5. Opret din familiar

```bash
cp persona-template/en.md ME.md
# Rediger ME.md ‚Äî giv det et navn og personlighed
```

### 6. K√∏r

```bash
./run.sh             # Tekstuel TUI (anbefales)
./run.sh --no-tui    # Simpel REPL
```

---

## Valg af en LLM

> **Anbefalet: Kimi K2.5** ‚Äî bedst agentisk pr√¶station testet indtil videre. L√¶gger m√¶rke til konteksten, stiller opf√∏lgende sp√∏rgsm√•l og handler autonomt p√• m√•der, som andre modeller ikke g√∏r. Prissat ligesom Claude Haiku.

| Platform | `PLATFORM=` | Standard model | Hvor man f√•r n√∏glen |
|----------|-------------|----------------|---------------------|
| **Moonshot Kimi K2.5** | `kimi` | `kimi-k2.5` | [platform.moonshot.ai](https://platform.moonshot.ai) |
| Z.AI GLM | `glm` | `glm-4.6v` | [api.z.ai](https://api.z.ai) |
| Anthropic Claude | `anthropic` | `claude-haiku-4-5-20251001` | [console.anthropic.com](https://console.anthropic.com) |
| Google Gemini | `gemini` | `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com) |
| OpenAI | `openai` | `gpt-4o-mini` | [platform.openai.com](https://platform.openai.com) |
| OpenAI-kompatibel (Ollama, vllm‚Ä¶) | `openai` + `BASE_URL=` | ‚Äî | ‚Äî |
| OpenRouter.ai (multi-provider) | `openai` + `BASE_URL=https://openrouter.ai/api/v1` | ‚Äî | [openrouter.ai](https://openrouter.ai) |
| **CLI v√¶rkt√∏j** (claude -p, ollama‚Ä¶) | `cli` | (kommandoen) | ‚Äî |

**Kimi K2.5 `.env` eksempel:**
```env
PLATFORM=kimi
API_KEY=sk-...   # fra platform.moonshot.ai
AGENT_NAME=Yukine
```

**Z.AI GLM `.env` eksempel:**
```env
PLATFORM=glm
API_KEY=...   # fra api.z.ai
MODEL=glm-4.6v   # visionsaktiveret; glm-4.7 / glm-5 = tekst-eller
AGENT_NAME=Yukine
```

**Google Gemini `.env` eksempel:**
```env
PLATFORM=gemini
API_KEY=AIza...   # fra aistudio.google.com
MODEL=gemini-2.5-flash  # eller gemini-2.5-pro for h√∏jere kapabilitet
AGENT_NAME=Yukine
```

**OpenRouter.ai `.env` eksempel:**
```env
PLATFORM=openai
BASE_URL=https://openrouter.ai/api/v1
API_KEY=sk-or-...   # fra openrouter.ai
MODEL=mistralai/mistral-7b-instruct  # valgfri: angiv model
AGENT_NAME=Yukine
```

> **Bem√¶rk:** For at deaktivere lokale/NVIDIA modeller, skal du blot ikke s√¶tte `BASE_URL` til en lokal slutpunkt som `http://localhost:11434/v1`. Brug skyudbydere i stedet.

**CLI v√¶rkt√∏j `.env` eksempel:**
```env
PLATFORM=cli
MODEL=llm -m gemma3 {}        # llm CLI (https://llm.datasette.io) ‚Äî {} = prompt argument
# MODEL=ollama run gemma3:27b  # Ollama ‚Äî no {}, prompt g√•r via stdin
```

---

## MCP Servere

familiar-ai kan oprette forbindelse til enhver [MCP (Model Context Protocol)](https://modelcontextprotocol.io) server. Dette giver dig mulighed for at tilslutte ekstern hukommelse, filsystemadgang, webs√∏gefunktioner eller hvilket som helst andet v√¶rkt√∏j.

Konfigurer servere i `~/.familiar-ai.json` (samme format som Claude Code):

```json
{
  "mcpServers": {
    "filesystem": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user"]
    },
    "memory": {
      "type": "sse",
      "url": "http://localhost:3000/sse"
    }
  }
}
```

To transporttyper underst√∏ttes:
- **`stdio`**: start en lokal underproces (`command` + `args`)
- **`sse`**: opret forbindelse til en HTTP+SSE-server (`url`)

Overskriv konfigurationsfilens placering med `MCP_CONFIG=/path/to/config.json`.

---

## Hardware

familiar-ai fungerer med hvad som helst hardware, du har ‚Äî eller slet ingen.

| Del | Hvad det g√∏r | Eksempel | Kr√¶vet? |
|-----|--------------|----------|---------|
| Wi-Fi PTZ kamera | √òjne + nakke | Tapo C220 (~$30) | **Anbefalet** |
| USB webcam | √òjne (fast) | Ethvert UVC-kamera | **Anbefalet** |
| Robotst√∏vsuger | Ben | Ethvert Tuya-kompatibelt model | Nej |
| PC / Raspberry Pi | Hjerne | Alt, der k√∏rer Python | **Ja** |

> **Et kamera er st√¶rkt anbefalet.** Uden et kan familiar-ai stadig tale ‚Äî men det kan ikke se verden, hvilket er lidt af det hele.

### Minimal ops√¶tning (ingen hardware)

Vil du bare pr√∏ve det? Du skal kun bruge en API-n√∏gle:

```env
PLATFORM=kimi
API_KEY=sk-...
```

K√∏r `./run.sh` og start med at chatte. Tilf√∏j hardware, som du g√•r.

### Wi-Fi PTZ kamera (Tapo C220)

1. I Tapo-appen: **Indstillinger ‚Üí Avanceret ‚Üí Kamera-konto** ‚Äî opret en lokal konto (ikke TP-Link-konto)
2. Find kameraets IP i din routers enhedsoversigt
3. S√¶t i `.env`:
   ```env
   CAMERA_HOST=192.168.1.xxx
   CAMERA_USER=your-local-user
   CAMERA_PASS=your-local-pass
   ```

### Stemme (ElevenLabs)

1. F√• en API-n√∏gle p√• [elevenlabs.io](https://elevenlabs.io/)
2. S√¶t i `.env`:
   ```env
   ELEVENLABS_API_KEY=sk_...
   ELEVENLABS_VOICE_ID=...   # valgfri, bruger standardstemmen hvis udeladt
   ```

Der er to afspilningsdestinationer, styret af `TTS_OUTPUT`:

```env
TTS_OUTPUT=local    # PC-h√∏jttaler (standard)
TTS_OUTPUT=remote   # kun kamera-h√∏jttaler
TTS_OUTPUT=both     # kamera-h√∏jttaler + PC-h√∏jttaler samtidig
```

#### A) Kamera h√∏jttaler (via go2rtc)

S√¶t `TTS_OUTPUT=remote` (eller `both`). Kr√¶ver [go2rtc](https://github.com/AlexxIT/go2rtc/releases):

1. Download den bin√¶re fil fra [udgivelsessiden](https://github.com/AlexxIT/go2rtc/releases):
   - Linux/macOS: `go2rtc_linux_amd64` / `go2rtc_darwin_amd64`
   - **Windows: `go2rtc_win64.exe`**

2. Placer og omd√∏b den:
   ```
   # Linux / macOS
   ~/.cache/embodied-claude/go2rtc/go2rtc          # chmod +x kr√¶vet

   # Windows
   %USERPROFILE%\.cache\embodied-claude\go2rtc\go2rtc.exe
   ```

3. Opret `go2rtc.yaml` i den samme mappe:
   ```yaml
   streams:
     tapo_cam:
       - rtsp://YOUR_CAM_USER:YOUR_CAM_PASS@YOUR_CAM_IP/stream1
   ```
   Brug de lokale kamera konto legitimationsoplysninger (ikke din TP-Link cloud konto).

4. familiar-ai starter go2rtc automatisk ved lancering. Hvis dit kamera underst√∏tter tovejslyd (tilbagemelding), afspilles stemmen fra kameraets h√∏jttaler.

#### B) Lokal PC h√∏jttaler

Standard (`TTS_OUTPUT=local`). Fors√∏ger spillere i r√¶kkef√∏lge: **paplay** ‚Üí **mpv** ‚Üí **ffplay**. Bruges ogs√• som en fallback n√•r `TTS_OUTPUT=remote` og go2rtc ikke er tilg√¶ngelig.

| OS | Install√©r |
|----|-----------|
| macOS | `brew install mpv` |
| Ubuntu / Debian | `sudo apt install mpv` (eller `paplay` via `pulseaudio-utils`) |
| WSL2 / WSLg | `sudo apt install pulseaudio-utils` ‚Äî s√¶t `PULSE_SERVER=unix:/mnt/wslg/PulseServer` i `.env` |
| Windows | [mpv.io/installation](https://mpv.io/installation/) ‚Äî download og tilf√∏j til PATH, **eller** `winget install ffmpeg` |

> Hvis der ikke er nogen lydafspiller tilg√¶ngelig, genereres talen stadig ‚Äî den vil bare ikke afspille.

### Stemme input (Realtime STT)

S√¶t `REALTIME_STT=true` i `.env` for altid-on, hands-free stemmeinput:

```env
REALTIME_STT=true
ELEVENLABS_API_KEY=sk_...   # samme n√∏gle som TTS
```

familiar-ai streamer mikrofonlyd til ElevenLabs Scribe v2 og auto-committer manuskripter, n√•r du stopper med at tale. Ingen knaptryk kr√¶ves. Sameksisterer med push-to-talk tilstand (Ctrl+T).

---

## TUI

familiar-ai inkluderer en terminalbrugerflade bygget med [Textual](https://textual.textualize.io/):

- Rulbar samtalehistorik med live streamende tekst
- Tabfuldf√∏relse for `/quit`, `/clear`
- Afbryd agenten midt i tankegangen ved at skrive, mens den t√¶nker
- **Samtalelogg** gemmes automatisk til `~/.cache/familiar-ai/chat.log`

For at f√∏lge loggen i et andet terminal (nyttigt til kopier-og-inds√¶t):
```bash
tail -f ~/.cache/familiar-ai/chat.log
```

---

## Persona (ME.md)

Din familiars personlighed findes i `ME.md`. Denne fil ignoreres af git ‚Äî den er kun din.

Se [`persona-template/en.md`](./persona-template/en.md) for et eksempel, eller [`persona-template/ja.md`](./persona-template/ja.md) for en japansk version.

---

## FAQ

**Q: Fungerer det uden en GPU?**  
Ja. Embedding modellen (multilingual-e5-small) fungerer fint p√• CPU. En GPU g√∏r det hurtigere, men det er ikke n√∏dvendigt.

**Q: Kan jeg bruge et kamera andet end Tapo?**  
Ethvert kamera, der underst√∏tter ONVIF + RTSP, burde fungere. Tapo C220 er det, vi har testet med.

**Q: Bliver mine data sendt nogen steder?**  
Billeder og tekst sendes til din valgte LLM API til behandling. Minder gemmes lokalt i `~/.familiar_ai/`.

**Q: Hvorfor skriver agenten `Ôºà...Ôºâ` i stedet for at tale?**  
S√∏rg for at `ELEVENLABS_API_KEY` er indstillet. Uden det er stemmen deaktiveret, og agenten falder tilbage p√• tekst.

## Teknisk baggrund

Nysgerrig efter hvordan det fungerer? Se [docs/technical.md](./docs/technical.md) for forskningen og designbeslutningerne bag familiar-ai ‚Äî ReAct, SayCan, Reflexion, Voyager, √∏nskesystemet, og meget mere.

---

## Bidrag

familiar-ai er et √•bent eksperiment. Hvis noget af dette resonerer med dig ‚Äî teknisk eller filosofisk ‚Äî er bidrag meget velkomne.

**Gode steder at starte:**

| Omr√•de | Hvad der er behov for |
|--------|----------------------|
| Ny hardware | Support for flere kameraer (RTSP, IP Webcam), mikrofoner, aktuatorer |
| Nye v√¶rkt√∏jer | Webs√∏gemaskine, hjemmeautomatisering, kalender, hvad som helst via MCP |
| Nye backend | Enhver LLM eller lokal model, der passer til `stream_turn` interface |
| Persona skabeloner | ME.md skabeloner for forskellige sprog og personligheder |
| Forskning | Bedre √∏nskemodeller, hukommelsesfremh√¶vning, theory-of-mind prompting |
| Dokumentation | Tutorials, walkthroughs, overs√¶ttelser |

Se [CONTRIBUTING.md](./CONTRIBUTING.md) for dev ops√¶tning, kodestil, og PR retningslinjer.

Hvis du er usikker p√•, hvor du skal starte, [√•bn et issue](https://github.com/lifemate-ai/familiar-ai/issues) ‚Äî glad for at pege dig i den rigtige retning.

---

## Licens

[MIT](./LICENSE)
