# familiar-ai üêæ

**Uma IA que vive ao seu lado** ‚Äî com olhos, voz, pernas e mem√≥ria.

[![Lint](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml/badge.svg)](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)
[![GitHub Sponsors](https://img.shields.io/github/sponsors/kmizu?style=flat&logo=github&color=ea4aaa)](https://github.com/sponsors/kmizu)

[‚Üí English README](../README.md)

---

[![Demo video](https://img.youtube.com/vi/hiR9uWRnjt4/0.jpg)](https://youtube.com/shorts/hiR9uWRnjt4)

familiar-ai √© um companheiro de IA que vive na sua casa. Configure em minutos. Nenhuma codifica√ß√£o necess√°ria.

Ele percebe o mundo real atrav√©s de c√¢meras, se move em um corpo rob√¥, fala em voz alta e se lembra do que v√™. D√™-lhe um nome, escreva sua personalidade e deixe-o viver com voc√™.

## O que ele pode fazer

- üëÅ **Ver** ‚Äî captura imagens de uma c√¢mera PTZ Wi-Fi ou webcam USB
- üîÑ **Olhar ao redor** ‚Äî movimenta e inclina a c√¢mera para explorar os arredores
- ü¶ø **Mover** ‚Äî dirige um aspirador rob√¥ para circular pelo ambiente
- üó£ **Falar** ‚Äî conversa via ElevenLabs TTS
- üéô **Ouvir** ‚Äî entrada de voz sem uso das m√£os via ElevenLabs Realtime STT (opcional)
- üß† **Lembrar** ‚Äî armazena ativamente e recupera mem√≥rias com busca sem√¢ntica (SQLite + embeddings)
- ü´Ä **Teoria da Mente** ‚Äî toma a perspectiva da outra pessoa antes de responder
- üí≠ **Desejo** ‚Äî possui suas pr√≥prias motiva√ß√µes internas que desencadeiam comportamento aut√¥nomo

## Como funciona

familiar-ai executa um loop [ReAct](https://arxiv.org/abs/2210.03629) alimentado pela sua escolha de LLM. Ele percebe o mundo atrav√©s de ferramentas, pensa sobre o que fazer a seguir e age ‚Äî assim como uma pessoa faria.

```
entrada do usu√°rio
  ‚Üí pensar ‚Üí agir (c√¢mera / mover / falar / lembrar) ‚Üí observar ‚Üí pensar ‚Üí ...
```

Quando ocioso, age de acordo com seus pr√≥prios desejos: curiosidade, desejo de olhar para fora, saudade da pessoa com quem vive.

## Come√ßando

### 1. Instalar uv

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Instalar ffmpeg

ffmpeg √© **obrigat√≥rio** para captura de imagens da c√¢mera e reprodu√ß√£o de √°udio.

| SO | Comando |
|----|---------|
| macOS | `brew install ffmpeg` |
| Ubuntu / Debian | `sudo apt install ffmpeg` |
| Fedora / RHEL | `sudo dnf install ffmpeg` |
| Arch Linux | `sudo pacman -S ffmpeg` |
| Windows | `winget install ffmpeg` ‚Äî ou baixe de [ffmpeg.org](https://ffmpeg.org/download.html) e adicione ao PATH |
| Raspberry Pi | `sudo apt install ffmpeg` |

Verifique: `ffmpeg -version`

### 3. Clonar e instalar

```bash
git clone https://github.com/lifemate-ai/familiar-ai
cd familiar-ai
uv sync
```

### 4. Configurar

```bash
cp .env.example .env
# Edite .env com suas configura√ß√µes
```

**Requisitos m√≠nimos:**

| Vari√°vel | Descri√ß√£o |
|----------|-------------|
| `PLATFORM` | `anthropic` (padr√£o) \| `gemini` \| `openai` \| `kimi` \| `glm` |
| `API_KEY` | Sua chave de API para a plataforma escolhida |

**Opcional:**

| Vari√°vel | Descri√ß√£o |
|----------|-------------|
| `MODEL` | Nome do modelo (valores padr√£o sensatos por plataforma) |
| `AGENT_NAME` | Nome exibido na TUI (ex: `Yukine`) |
| `CAMERA_HOST` | Endere√ßo IP da sua c√¢mera ONVIF/RTSP |
| `CAMERA_USER` / `CAMERA_PASS` | Credenciais da c√¢mera |
| `ELEVENLABS_API_KEY` | Para sa√≠da de voz ‚Äî [elevenlabs.io](https://elevenlabs.io/) |
| `REALTIME_STT` | `true` para ativar entrada de voz sem m√£os sempre ativa (requer `ELEVENLABS_API_KEY`) |
| `TTS_OUTPUT` | Onde reproduzir o √°udio: `local` (alto-falante do PC, padr√£o) \| `remote` (alto-falante da c√¢mera) \| `both` |
| `THINKING_MODE` | Apenas Anthropic ‚Äî `auto` (padr√£o) \| `adaptive` \| `extended` \| `disabled` |
| `THINKING_EFFORT` | Esfor√ßo de pensamento adaptativo: `high` (padr√£o) \| `medium` \| `low` \| `max` (apenas Opus 4.6) |

### 5. Crie seu familiar

```bash
cp persona-template/en.md ME.md
# Edite ME.md ‚Äî d√™ um nome e personalidade a ele
```

### 6. Executar

```bash
./run.sh             # TUI textual (recomendado)
./run.sh --no-tui    # REPL simples
```

---

## Escolhendo um LLM

> **Recomendado: Kimi K2.5** ‚Äî melhor desempenho agente testado at√© agora. Nota contexto, faz perguntas de acompanhamento e age autonomamente de maneiras que outros modelos n√£o fazem. Pre√ßo semelhante ao Claude Haiku.

| Plataforma | `PLATFORM=` | Modelo padr√£o | Onde obter a chave |
|------------|------------|---------------|--------------------|
| **Moonshot Kimi K2.5** | `kimi` | `kimi-k2.5` | [platform.moonshot.ai](https://platform.moonshot.ai) |
| Z.AI GLM | `glm` | `glm-4.6v` | [api.z.ai](https://api.z.ai) |
| Anthropic Claude | `anthropic` | `claude-haiku-4-5-20251001` | [console.anthropic.com](https://console.anthropic.com) |
| Google Gemini | `gemini` | `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com) |
| OpenAI | `openai` | `gpt-4o-mini` | [platform.openai.com](https://platform.openai.com) |
| OpenAI-compatible (Ollama, vllm‚Ä¶) | `openai` + `BASE_URL=` | ‚Äî | ‚Äî |
| OpenRouter.ai (multi-provedor) | `openai` + `BASE_URL=https://openrouter.ai/api/v1` | ‚Äî | [openrouter.ai](https://openrouter.ai) |
| **Ferramenta CLI** (claude -p, ollama‚Ä¶) | `cli` | (o comando) | ‚Äî |

**Exemplo `.env` para Kimi K2.5:**
```env
PLATFORM=kimi
API_KEY=sk-...   # da platform.moonshot.ai
AGENT_NAME=Yukine
```

**Exemplo `.env` para Z.AI GLM:**
```env
PLATFORM=glm
API_KEY=...   # da api.z.ai
MODEL=glm-4.6v   # habilitado para vis√£o; glm-4.7 / glm-5 = apenas texto
AGENT_NAME=Yukine
```

**Exemplo `.env` para Google Gemini:**
```env
PLATFORM=gemini
API_KEY=AIza...   # da aistudio.google.com
MODEL=gemini-2.5-flash  # ou gemini-2.5-pro para maior capacidade
AGENT_NAME=Yukine
```

**Exemplo `.env` para OpenRouter.ai:**
```env
PLATFORM=openai
BASE_URL=https://openrouter.ai/api/v1
API_KEY=sk-or-...   # de openrouter.ai
MODEL=mistralai/mistral-7b-instruct  # opcional: especificar modelo
AGENT_NAME=Yukine
```

> **Nota:** Para desativar modelos locais/NVIDIA, simplesmente n√£o defina `BASE_URL` para um ponto final local como `http://localhost:11434/v1`. Use provedores de nuvem em vez disso.

**Exemplo `.env` para ferramenta CLI:**
```env
PLATFORM=cli
MODEL=llm -m gemma3 {}        # llm CLI (https://llm.datasette.io) ‚Äî {} = argumento do prompt
# MODEL=ollama run gemma3:27b  # Ollama ‚Äî sem {}, prompt vai via stdin
```

---

## Servidores MCP

familiar-ai pode conectar a qualquer servidor [MCP (Modelo Contexto Protocolo)](https://modelcontextprotocol.io). Isso permite que voc√™ conecte mem√≥ria externa, acesso ao sistema de arquivos, busca na web ou qualquer outra ferramenta.

Configurar servidores em `~/.familiar-ai.json` (mesmo formato que o Claude Code):

```json
{
  "mcpServers": {
    "filesystem": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user"]
    },
    "memory": {
      "type": "sse",
      "url": "http://localhost:3000/sse"
    }
  }
}
```

Dois tipos de transporte s√£o suportados:
- **`stdio`**: inicia um subprocesso local (`command` + `args`)
- **`sse`**: conecta a um servidor HTTP+SSE (`url`)

Substitua a localiza√ß√£o do arquivo de configura√ß√£o com `MCP_CONFIG=/path/to/config.json`.

---

## Hardware

familiar-ai funciona com qualquer hardware que voc√™ tiver ‚Äî ou nenhum.

| Parte | O que faz | Exemplo | Necess√°rio? |
|------|-------------|---------|-----------|
| C√¢mera PTZ Wi-Fi | Olhos + pesco√ßo | Tapo C220 (~$30) | **Recomendado** |
| Webcam USB | Olhos (fixo) | Qualquer c√¢mera UVC | **Recomendado** |
| Aspirador rob√¥ | Pernas | Qualquer modelo compat√≠vel com Tuya | N√£o |
| PC / Raspberry Pi | C√©rebro | Qualquer um que execute Python | **Sim** |

> **Uma c√¢mera √© fortemente recomendada.** Sem uma, familiar-ai ainda pode falar ‚Äî mas n√£o pode ver o mundo, o que √© meio que o objetivo.

### Configura√ß√£o m√≠nima (sem hardware)

S√≥ quer testar? Voc√™ s√≥ precisa de uma chave de API:

```env
PLATFORM=kimi
API_KEY=sk-...
```

Execute `./run.sh` e comece a conversar. Adicione hardware conforme necess√°rio.

### C√¢mera PTZ Wi-Fi (Tapo C220)

1. No aplicativo Tapo: **Configura√ß√µes ‚Üí Avan√ßado ‚Üí Conta da C√¢mera** ‚Äî crie uma conta local (n√£o a conta TP-Link)
2. Encontre o IP da c√¢mera na lista de dispositivos do seu roteador
3. Defina em `.env`:
   ```env
   CAMERA_HOST=192.168.1.xxx
   CAMERA_USER=seu-usu√°rio-local
   CAMERA_PASS=sua-senha-local
   ```

### Voz (ElevenLabs)

1. Obtenha uma chave de API em [elevenlabs.io](https://elevenlabs.io/)
2. Defina em `.env`:
   ```env
   ELEVENLABS_API_KEY=sk_...
   ELEVENLABS_VOICE_ID=...   # opcional, usa a voz padr√£o se omitido
   ```

Existem dois destinos de reprodu√ß√£o, controlados por `TTS_OUTPUT`:

```env
TTS_OUTPUT=local    # alto-falante do PC (padr√£o)
TTS_OUTPUT=remote   # alto-falante da c√¢mera apenas
TTS_OUTPUT=both     # alto-falante da c√¢mera + alto-falante do PC simultaneamente
```

#### A) Alto-falante da c√¢mera (via go2rtc)

Defina `TTS_OUTPUT=remote` (ou `both`). Requer [go2rtc](https://github.com/AlexxIT/go2rtc/releases):

1. Baixe o bin√°rio da [p√°gina de lan√ßamentos](https://github.com/AlexxIT/go2rtc/releases):
   - Linux/macOS: `go2rtc_linux_amd64` / `go2rtc_darwin_amd64`
   - **Windows: `go2rtc_win64.exe`**

2. Coloque e renomeie:
   ```
   # Linux / macOS
   ~/.cache/embodied-claude/go2rtc/go2rtc          # chmod +x necess√°rio

   # Windows
   %USERPROFILE%\.cache\embodied-claude\go2rtc\go2rtc.exe
   ```

3. Crie `go2rtc.yaml` no mesmo diret√≥rio:
   ```yaml
   streams:
     tapo_cam:
       - rtsp://SEU_USU√ÅRIO_CAM:SUA_SENHA_CAM@SEU_IP_CAM/stream1
   ```
   Use as credenciais da conta local da c√¢mera (n√£o sua conta em nuvem TP-Link).

4. familiar-ai inicia go2rtc automaticamente na inicializa√ß√£o. Se sua c√¢mera suportar √°udio bidirecional (canal reverso), a voz √© reproduzida pelo alto-falante da c√¢mera.

#### B) Alto-falante local do PC

O padr√£o (`TTS_OUTPUT=local`). Tenta players na ordem: **paplay** ‚Üí **mpv** ‚Üí **ffplay**. Tamb√©m √© usado como uma op√ß√£o de fallback quando `TTS_OUTPUT=remote` e go2rtc n√£o est√° dispon√≠vel.

| SO | Instalar |
|----|---------|
| macOS | `brew install mpv` |
| Ubuntu / Debian | `sudo apt install mpv` (ou `paplay` via `pulseaudio-utils`) |
| WSL2 / WSLg | `sudo apt install pulseaudio-utils` ‚Äî defina `PULSE_SERVER=unix:/mnt/wslg/PulseServer` em `.env` |
| Windows | [mpv.io/installation](https://mpv.io/installation/) ‚Äî baixe e adicione ao PATH, **ou** `winget install ffmpeg` |

> Se nenhum player de √°udio estiver dispon√≠vel, a fala ainda ser√° gerada ‚Äî apenas n√£o ser√° reproduzida.

### Entrada de voz (Realtime STT)

Defina `REALTIME_STT=true` em `.env` para entrada de voz sempre ativa, sem as m√£os:

```env
REALTIME_STT=true
ELEVENLABS_API_KEY=sk_...   # mesma chave que TTS
```

familiar-ai transmite √°udio do microfone para ElevenLabs Scribe v2 e comete automaticamente a transcri√ß√£o quando voc√™ para de falar. Nenhum bot√£o √© necess√°rio. Coexiste com o modo de empurrar para falar (Ctrl+T).

---

## TUI

familiar-ai inclui uma interface terminal constru√≠da com [Textual](https://textual.textualize.io/):

- Hist√≥rico de conversas rol√°vel com texto em tempo real
- Completamento de aba para `/quit`, `/clear`
- Interrompa o agente no meio da resposta digitando enquanto ele est√° pensando
- **Registro de conversa** salvo automaticamente em `~/.cache/familiar-ai/chat.log`

Para seguir o registro em outro terminal (√∫til para copiar-colar):
```bash
tail -f ~/.cache/familiar-ai/chat.log
```

---

## Persona (ME.md)

A personalidade do seu familiar vive em `ME.md`. Este arquivo √© gitignored ‚Äî √© s√≥ seu.

Veja [`persona-template/en.md`](./persona-template/en.md) para um exemplo, ou [`persona-template/ja.md`](./persona-template/ja.md) para uma vers√£o em japon√™s.

---

## FAQ

**P: Funciona sem uma GPU?**  
Sim. O modelo de embeddings (multilingual-e5-small) funciona bem em CPU. Uma GPU torna mais r√°pido, mas n√£o √© necess√°ria.

**P: Posso usar uma c√¢mera diferente da Tapo?**  
Qualquer c√¢mera que suporte ONVIF + RTSP deve funcionar. Tapo C220 √© com a qual testamos.

**P: Meus dados s√£o enviados para algum lugar?**  
Imagens e textos s√£o enviados para a API do LLM escolhido para processamento. Mem√≥rias s√£o armazenadas localmente em `~/.familiar_ai/`.

**P: Por que o agente escreve `Ôºà...Ôºâ` em vez de falar?**  
Certifique-se de que `ELEVENLABS_API_KEY` esteja definido. Sem isso, a voz √© desativada e o agente recorre ao texto.

## Contexto t√©cnico

Curioso sobre como funciona? Veja [docs/technical.md](./docs/technical.md) para as decis√µes de pesquisa e design por tr√°s do familiar-ai ‚Äî ReAct, SayCan, Reflexion, Voyager, o sistema de desejo e mais.

---

## Contribuindo

familiar-ai √© um experimento aberto. Se algo disso ressoa com voc√™ ‚Äî tecnicamente ou filosoficamente ‚Äî contribui√ß√µes s√£o muito bem-vindas.

**Bons lugares para come√ßar:**

| √Årea | O que √© necess√°rio |
|------|---------------|
| Novo hardware | Suporte para mais c√¢meras (RTSP, IP Webcam), microfones, atuadores |
| Novas ferramentas | Busca na web, automa√ß√£o residencial, calend√°rio, qualquer coisa via MCP |
| Novos backends | Qualquer LLM ou modelo local que se encaixe na interface `stream_turn` |
| Modelos de persona | Modelos ME.md para diferentes idiomas e personalidades |
| Pesquisa | Melhores modelos de desejo, recupera√ß√£o de mem√≥ria, prompting da teoria da mente |
| Documenta√ß√£o | Tutoriais, guias, tradu√ß√µes |

Veja [CONTRIBUTING.md](./CONTRIBUTING.md) para configura√ß√£o de desenvolvimento, estilo de c√≥digo e diretrizes de PR.

Se voc√™ n√£o sabe por onde come√ßar, [abra uma issue](https://github.com/lifemate-ai/familiar-ai/issues) ‚Äî ficaremos felizes em te direcionar.  

---

## Licen√ßa

[MIT](./LICENSE)
