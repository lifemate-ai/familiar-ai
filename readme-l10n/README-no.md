```markdown
# familiar-ai üêæ

**En AI som lever sammen med deg** ‚Äî med √∏yne, stemme, bein og minne.

[![Lint](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml/badge.svg)](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)
[![GitHub Sponsors](https://img.shields.io/github/sponsors/kmizu?style=flat&logo=github&color=ea4aaa)](https://github.com/sponsors/kmizu)

üåç [Tilgjengelig p√• 74 spr√•k](./SUPPORTED_LANGUAGES.md)

---

[![Demo video](https://img.youtube.com/vi/hiR9uWRnjt4/0.jpg)](https://youtube.com/shorts/hiR9uWRnjt4)

familiar-ai er en AI-kompanjong som bor i hjemmet ditt. Sett det opp p√• minutter. Ingen koding n√∏dvendig.

Det oppfatter den virkelige verden gjennom kameraer, beveger seg rundt p√• en robotkropp, snakker h√∏yt og husker det den ser. Gi det et navn, skriv dens personlighet, og la det bo med deg.

## Hva den kan gj√∏re

- üëÅ **Se** ‚Äî tar bilder fra et Wi-Fi PTZ-kamera eller USB-webkamera
- üîÑ **Se rundt** ‚Äî panorering og neigen av kameraet for √• utforske omgivelsene
- ü¶ø **Bevege seg** ‚Äî kj√∏rer en robotst√∏vsuger for √• vandre rundt i rommet
- üó£ **Snakke** ‚Äî snakker via ElevenLabs TTS
- üéô **Lytte** ‚Äî hands-free talebehandling via ElevenLabs Realtime STT (opt-in)
- üß† **Huske** ‚Äî lagrer aktivt og husker minner med semantisk s√∏k (SQLite + innb√∏ttinger)
- ü´Ä **Teori om sinn** ‚Äî tar den andre personens perspektiv f√∏r den svarer
- üí≠ **√ònske** ‚Äî har sine egne interne drivkrefter som utl√∏ser autonom atferd

## Hvordan det fungerer

familiar-ai kj√∏rer en [ReAct](https://arxiv.org/abs/2210.03629) l√∏kke drevet av ditt valg av LLM. Den oppfatter verden gjennom verkt√∏y, tenker p√• hva den skal gj√∏re neste, og handler ‚Äî akkurat som et menneske ville gjort.

```
user input
  ‚Üí think ‚Üí act (camera / move / speak / remember) ‚Üí observe ‚Üí think ‚Üí ...
```

N√•r den er inaktiv, handler den p√• sine egne √∏nsker: nysgjerrighet, √∏nsker √• se ut, savner personen den bor med.

## Komme i gang

### 1. Installer uv

**macOS / Linux / WSL2:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Windows (PowerShell):**
```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```
Eller: `winget install astral-sh.uv`

### 2. Installer ffmpeg

ffmpeg er **n√∏dvendig** for kamera bildedekning og lydavspilling.

| OS | Kommando |
|----|---------|
| macOS | `brew install ffmpeg` |
| Ubuntu / Debian | `sudo apt install ffmpeg` |
| Fedora / RHEL | `sudo dnf install ffmpeg` |
| Arch Linux | `sudo pacman -S ffmpeg` |
| Windows | `winget install ffmpeg` ‚Äî eller last ned fra [ffmpeg.org](https://ffmpeg.org/download.html) og legg til PATH |
| Raspberry Pi | `sudo apt install ffmpeg` |

Bekreft: `ffmpeg -version`

### 3. Klon og installer

```bash
git clone https://github.com/lifemate-ai/familiar-ai
cd familiar-ai
uv sync
```

### 4. Konfigurer

```bash
cp .env.example .env
# Rediger .env med innstillingene dine
```

**Minimum p√•krevd:**

| Variabel | Beskrivelse |
|----------|-------------|
| `PLATFORM` | `anthropic` (standard) \| `gemini` \| `openai` \| `kimi` \| `glm` |
| `API_KEY` | Din API-n√∏kkel for den valgte plattformen |

**Valgfritt:**

| Variabel | Beskrivelse |
|----------|-------------|
| `MODEL` | Modellnavn (fornuftige standarder per plattform) |
| `AGENT_NAME` | Vist navn i TUI (f.eks. `Yukine`) |
| `CAMERA_HOST` | IP-adresse til ditt ONVIF/RTSP-kamera |
| `CAMERA_USER` / `CAMERA_PASS` | Kamera-legitimasjon |
| `ELEVENLABS_API_KEY` | For stemmeutgang ‚Äî [elevenlabs.io](https://elevenlabs.io/) |
| `REALTIME_STT` | `true` for √• aktivere alltid-p√• hands-free taledekning (krever `ELEVENLABS_API_KEY`) |
| `TTS_OUTPUT` | Hvor lyden skal spilles: `local` (PC-h√∏yttaler, standard) \| `remote` (kamera-h√∏yttaler) \| `both` |
| `THINKING_MODE` | Kun Anthropic ‚Äî `auto` (standard) \| `adaptive` \| `extended` \| `disabled` |
| `THINKING_EFFORT` | Adaptiv tenkeinnsats: `high` (standard) \| `medium` \| `low` \| `max` (Kun Opus 4.6) |

### 5. Lag din familiar

```bash
cp persona-template/en.md ME.md
# Rediger ME.md ‚Äî gi det et navn og personlighet
```

### 6. Kj√∏r

**macOS / Linux / WSL2:**
```bash
./run.sh             # Tekstlig TUI (anbefalt)
./run.sh --no-tui    # Plain REPL
```

**Windows:**
```bat
run.bat              # Tekstlig TUI (anbefalt)
run.bat --no-tui     # Plain REPL
```

---

## Velge en LLM

> **Anbefalt: Kimi K2.5** ‚Äî beste agentiske ytelse som er testet s√• langt. Legger merke til konteksten, stiller oppf√∏lgingssp√∏rsm√•l og handler autonomt p√• m√•ter andre modeller ikke gj√∏r. Priset likt som Claude Haiku.

| Plattform | `PLATFORM=` | Standardmodell | Hvor f√• n√∏kkel |
|----------|------------|---------------|-----------------|
| **Moonshot Kimi K2.5** | `kimi` | `kimi-k2.5` | [platform.moonshot.ai](https://platform.moonshot.ai) |
| Z.AI GLM | `glm` | `glm-4.6v` | [api.z.ai](https://api.z.ai) |
| Anthropic Claude | `anthropic` | `claude-haiku-4-5-20251001` | [console.anthropic.com](https://console.anthropic.com) |
| Google Gemini | `gemini` | `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com) |
| OpenAI | `openai` | `gpt-4o-mini` | [platform.openai.com](https://platform.openai.com) |
| OpenAI-kompatibel (Ollama, vllm‚Ä¶) | `openai` + `BASE_URL=` | ‚Äî | ‚Äî |
| OpenRouter.ai (multi-provider) | `openai` + `BASE_URL=https://openrouter.ai/api/v1` | ‚Äî | [openrouter.ai](https://openrouter.ai) |
| **CLI-verkt√∏y** (claude -p, ollama‚Ä¶) | `cli` | (kommandolinjen) | ‚Äî |

**Kimi K2.5 `.env` eksempel:**
```env
PLATFORM=kimi
API_KEY=sk-...   # fra plattform.moonshot.ai
AGENT_NAME=Yukine
```

**Z.AI GLM `.env` eksempel:**
```env
PLATFORM=glm
API_KEY=...   # fra api.z.ai
MODEL=glm-4.6v   # vision-enabled; glm-4.7 / glm-5 = tekstkun
AGENT_NAME=Yukine
```

**Google Gemini `.env` eksempel:**
```env
PLATFORM=gemini
API_KEY=AIza...   # fra aistudio.google.com
MODEL=gemini-2.5-flash  # eller gemini-2.5-pro for h√∏yere kapasitet
AGENT_NAME=Yukine
```

**OpenRouter.ai `.env` eksempel:**
```env
PLATFORM=openai
BASE_URL=https://openrouter.ai/api/v1
API_KEY=sk-or-...   # fra openrouter.ai
MODEL=mistralai/mistral-7b-instruct  # valgfritt: spesifiser modell
AGENT_NAME=Yukine
```

> **Merk:** For √• deaktivere lokale/NVIDIA-modeller, sett bare ikke `BASE_URL` til en lokal endepunkt som `http://localhost:11434/v1`. Bruk skyl√∏sninger i stedet.

**CLI-verkt√∏y `.env` eksempel:**
```env
PLATFORM=cli
MODEL=llm -m gemma3 {}        # llm CLI (https://llm.datasette.io) ‚Äî {} = prompt-argument
# MODEL=ollama run gemma3:27b  # Ollama ‚Äî ingen {}, prompt g√•r via stdin
```

---

## MCP-servere

familiar-ai kan koble seg til hvilken som helst [MCP (Model Context Protocol)](https://modelcontextprotocol.io) server. Dette lar deg koble til ekstern minne, filsystemtilgang, netts√∏k eller noe annet verkt√∏y.

Konfigurer servere i `~/.familiar-ai.json` (samme format som Claude Code):

```json
{
  "mcpServers": {
    "filesystem": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user"]
    },
    "memory": {
      "type": "sse",
      "url": "http://localhost:3000/sse"
    }
  }
}
```

To transporttyper st√∏ttes:
- **`stdio`**: starte en lokal undertask (`command` + `args`)
- **`sse`**: koble til en HTTP+SSE server (`url`)

Overskriv konfigurasjonsfilens plassering med `MCP_CONFIG=/path/to/config.json`.

---

## Maskinvare

familiar-ai fungerer med hvilken som helst maskinvare du har ‚Äî eller ingen i det hele tatt.

| Del | Hva den gj√∏r | Eksempel | N√∏dvendig? |
|------|-------------|---------|-----------|
| Wi-Fi PTZ-kamera | √òyne + nakke | Tapo C220 (~$30) | **Anbefalt** |
| USB-webkamera | √òyne (fast) | Enhvilken UVC-kamera | **Anbefalt** |
| Robotst√∏vsuger | Bein | Enhvilken Tuya-kompatibel modell | Nei |
| PC / Raspberry Pi | Hjerne | Alt som kj√∏rer Python | **Ja** |

> **Et kamera anbefales sterkt.** Uten et kan familiar-ai fortsatt snakke ‚Äî men den kan ikke se verden, som er liksom hele poenget.

### Minimal oppsett (ingen maskinvare)

Bare vil pr√∏ve det? Du trenger bare en API-n√∏kkel:

```env
PLATFORM=kimi
API_KEY=sk-...
```

Kj√∏r `./run.sh` (macOS/Linux/WSL2) eller `run.bat` (Windows) og start chatting. Legg til maskinvare mens du g√•r.

### Wi-Fi PTZ-kamera (Tapo C220)

1. I Tapo-appen: **Innstillinger ‚Üí Avansert ‚Üí Kamera-konto** ‚Äî opprett en lokal konto (ikke TP-Link-konto)
2. Finn kameraets IP i ruteren din enhetsliste
3. Sett i `.env`:
   ```env
   CAMERA_HOST=192.168.1.xxx
   CAMERA_USER=your-local-user
   CAMERA_PASS=your-local-pass
   ```

### Stemme (ElevenLabs)

1. F√• en API-n√∏kkel p√• [elevenlabs.io](https://elevenlabs.io/)
2. Sett i `.env`:
   ```env
   ELEVENLABS_API_KEY=sk_...
   ELEVENLABS_VOICE_ID=...   # valgfritt, bruker standard stemme hvis utelatt
   ```

Det er to avspillingsm√•lsteder, kontrollert av `TTS_OUTPUT`:

```env
TTS_OUTPUT=local    # PC-h√∏yttaler (standard)
TTS_OUTPUT=remote   # kun kamera-h√∏yttaler
TTS_OUTPUT=both     # kamera-h√∏yttaler + PC-h√∏yttaler samtidig
```

#### A) Kamera-h√∏yttaler (via go2rtc)

Sett `TTS_OUTPUT=remote` (eller `both`). Krever [go2rtc](https://github.com/AlexxIT/go2rtc/releases):

1. Last ned den kj√∏rbare filen fra [utgivelsessiden](https://github.com/AlexxIT/go2rtc/releases):
   - Linux/macOS: `go2rtc_linux_amd64` / `go2rtc_darwin_amd64`
   - **Windows: `go2rtc_win64.exe`**

2. Plasser og gi nytt navn:
   ```
   # Linux / macOS
   ~/.cache/embodied-claude/go2rtc/go2rtc          # chmod +x n√∏dvendig

   # Windows
   %USERPROFILE%\.cache\embodied-claude\go2rtc\go2rtc.exe
   ```

3. Lag `go2rtc.yaml` i samme katalog:
   ```yaml
   streams:
     tapo_cam:
       - rtsp://YOUR_CAM_USER:YOUR_CAM_PASS@YOUR_CAM_IP/stream1
   ```
   Bruk de lokale kamera-kontoinformasjonene (ikke din TP-Link sky-konto).

4. familiar-ai starter go2rtc automatisk ved oppstart. Hvis kameraet ditt st√∏tter toveis lyd (tilbakekanal), spilles stemmen fra kamera-h√∏yttaleren.

#### B) Lokal PC-h√∏yttaler

Den standard (`TTS_OUTPUT=local`). Pr√∏ver spillere i rekkef√∏lge: **paplay** ‚Üí **mpv** ‚Üí **ffplay**. Ogs√• brukt som en fallback n√•r `TTS_OUTPUT=remote` og go2rtc ikke er tilgjengelig.

| OS | Installer |
|----|---------|
| macOS | `brew install mpv` |
| Ubuntu / Debian | `sudo apt install mpv` (eller `paplay` via `pulseaudio-utils`) |
| WSL2 / WSLg | `sudo apt install pulseaudio-utils` ‚Äî sett `PULSE_SERVER=unix:/mnt/wslg/PulseServer` i `.env` |
| Windows | [mpv.io/installation](https://mpv.io/installation/) ‚Äî last ned og legg til PATH, **eller** `winget install ffmpeg` |

> Hvis ingen lydspiller er tilgjengelig, genereres tale fortsatt ‚Äî den vil bare ikke spille.

### Stemmeinnputt (Realtime STT)

Sett `REALTIME_STT=true` i `.env` for alltid-p√•, hands-free stemmeinput:

```env
REALTIME_STT=true
ELEVENLABS_API_KEY=sk_...   # samme n√∏kkel som TTS
```

familiar-ai str√∏mmer mikrofonlyd til ElevenLabs Scribe v2 og auto-registrerer transkripsjoner n√•r du pauser talingen. Ingen knapptrykk n√∏dvendig. Sameksisterer med push-to-talk-modus (Ctrl+T).

---

## TUI

familiar-ai inkluderer en terminal UI bygget med [Textual](https://textual.textualize.io/):

- Rullbar samtalehistorikk med live streaming tekst
- Tab-fullf√∏ring for `/quit`, `/clear`
- Avbryt agenten midt i turen ved √• skrive mens den tenker
- **Samtaleloggen** lagres automatisk til `~/.cache/familiar-ai/chat.log`

For √• f√∏lge loggen i en annen terminal (nyttig for kopi-og-lim):
```bash
tail -f ~/.cache/familiar-ai/chat.log
```

---

## Persona (ME.md)

Personligheten til din familiar ligger i `ME.md`. Denne filen er gitignored ‚Äî den er kun din.

Se [`persona-template/en.md`](./persona-template/en.md) for et eksempel, eller [`persona-template/ja.md`](./persona-template/ja.md) for en japansk versjon.

---

## FAQ

**Q: Fungerer det uten GPU?**
Ja. Inb√∏ttingsmodellen (multilingual-e5-small) kj√∏rer fint p√• CPU. En GPU gj√∏r det raskere, men er ikke n√∏dvendig.

**Q: Kan jeg bruke et kamera annet enn Tapo?**
Alle kameraer som st√∏tter ONVIF + RTSP b√∏r fungere. Tapo C220 er det vi testet med.

**Q: Blir dataene mine sendt noe sted?**
Bilder og tekst sendes til din valgte LLM API for behandling. Minner lagres lokalt i `~/.familiar_ai/`.

**Q: Hvorfor skriver agenten `Ôºà...Ôºâ` i stedet for √• snakke?**
S√∏rg for at `ELEVENLABS_API_KEY` er satt. Uten den er stemmen deaktivert og agenten faller tilbake til tekst.

## Teknisk bakgrunn

Nysgjerrig p√• hvordan det fungerer? Se [docs/technical.md](./docs/technical.md) for forskningen og designbeslutningene bak familiar-ai ‚Äî ReAct, SayCan, Reflexion, Voyager, √∏nskesystemet, og mer.

---

## Bidra

familiar-ai er et √•pent eksperiment. Hvis noen av dette resonnerer med deg ‚Äî teknisk eller filosofisk ‚Äî er bidrag veldig velkomne.

**Gode steder √• starte:**

| Omr√•de | Hva som trengs |
|------|---------------|
| Ny maskinvare | St√∏tte for flere kameraer (RTSP, IP Webcam), mikrofoner, aktuatorer |
| Nye verkt√∏y | Nett-s√∏k, hjemmeautomatisering, kalender, hva som helst via MCP |
| Nye backends | Enhvilken LLM eller lokal modell som passer til `stream_turn` grensesnittet |
| Persona-maler | ME.md maler for forskjellige spr√•k og personligheter |
| Forskning | Bedre √∏nsker modeller, minne henting, teori-om-sinn prompting |
| Dokumentasjon | Veiledninger, gjennomganger, oversettelser |

Se [CONTRIBUTING.md](./CONTRIBUTING.md) for utviklingsoppsett, kode-stil og PR-retningslinjer.

Hvis du er usikker p√• hvor du skal starte, [√•pne en sak](https://github.com/lifemate-ai/familiar-ai/issues) ‚Äî glad for √• peke deg i riktig retning.

---

## Lisens

[MIT](./LICENSE)
```
