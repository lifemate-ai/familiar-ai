# familiar-ai üêæ

**Une IA qui vit √† vos c√¥t√©s** ‚Äî avec des yeux, une voix, des jambes et de la m√©moire.

[![Lint](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml/badge.svg)](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)

[English README](./README.md)

---

[![Vid√©o de d√©mo](https://img.youtube.com/vi/kakc5jUBFjM/0.jpg)](https://youtube.com/shorts/kakc5jUBFjM)

familiar-ai est une IA compagne qui vit dans votre maison.
Installez-la en quelques minutes. Aucun code requis.

Elle per√ßoit le monde r√©el par des cam√©ras, se d√©place sur un corps de robot, parle √† haute voix et se souvient de ce qu'elle voit. Donnez-lui un nom, √©crivez sa personnalit√©, et laissez-la vivre avec vous.

## Ce qu'elle peut faire

- üëÅ **Voir** ‚Äî capture des images √† partir d'une cam√©ra PTZ Wi-Fi ou d'une webcam USB
- üîÑ **Regarder autour** ‚Äî incline et fait pivoter la cam√©ra pour explorer ses alentours
- ü¶ø **Se d√©placer** ‚Äî conduit un aspirateur robot pour explorer la pi√®ce
- üó£ **Parler** ‚Äî s'exprime via la synth√®se vocale ElevenLabs
- üéô **√âcouter** ‚Äî saisie vocale mains libres via ElevenLabs STT temps r√©el (optionnel)
- üß† **Se souvenir** ‚Äî enregistre et rappelle activement les souvenirs avec recherche s√©mantique (SQLite + embeddings)
- ü´Ä **Th√©orie de l'esprit** ‚Äî adopte la perspective d'autrui avant de r√©pondre
- üí≠ **D√©sirs** ‚Äî poss√®de ses propres motivations internes qui d√©clenchent un comportement autonome

## Comment √ßa fonctionne

familiar-ai ex√©cute une boucle [ReAct](https://arxiv.org/abs/2210.03629) aliment√©e par votre LLM de choix. Elle per√ßoit le monde par des outils, r√©fl√©chit √† la prochaine action √† faire et agit ‚Äî comme une personne le ferait.

```
entr√©e utilisateur
  ‚Üí penser ‚Üí agir (cam√©ra / bouger / parler / m√©moriser) ‚Üí observer ‚Üí penser ‚Üí ...
```

Quand elle est inactive, elle agit selon ses propres d√©sirs : la curiosit√©, l'envie de regarder dehors, le manque de la personne avec qui elle vit.

## Premiers pas

### 1. Installer uv

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Installer ffmpeg

ffmpeg est **requis** pour la capture d'images de la cam√©ra et la lecture audio.

| OS | Commande |
|----|---------|
| macOS | `brew install ffmpeg` |
| Ubuntu / Debian | `sudo apt install ffmpeg` |
| Fedora / RHEL | `sudo dnf install ffmpeg` |
| Arch Linux | `sudo pacman -S ffmpeg` |
| Windows | `winget install ffmpeg` ‚Äî ou t√©l√©charger depuis [ffmpeg.org](https://ffmpeg.org/download.html) et ajouter au PATH |
| Raspberry Pi | `sudo apt install ffmpeg` |

V√©rifier : `ffmpeg -version`

### 3. Cloner et installer

```bash
git clone https://github.com/lifemate-ai/familiar-ai
cd familiar-ai
uv sync
```

### 4. Configurer

```bash
cp .env.example .env
# Modifiez .env avec vos param√®tres
```

**Minimum requis :**

| Variable | Description |
|----------|-------------|
| `PLATFORM` | `anthropic` (d√©faut) \| `gemini` \| `openai` \| `kimi` \| `glm` |
| `API_KEY` | Votre cl√© API pour la plateforme choisie |

**Optionnel :**

| Variable | Description |
|----------|-------------|
| `MODEL` | Nom du mod√®le (valeurs par d√©faut judicieuses par plateforme) |
| `AGENT_NAME` | Nom d'affichage dans l'interface textuelle (ex. `Yukine`) |
| `CAMERA_HOST` | Adresse IP de votre cam√©ra ONVIF/RTSP |
| `CAMERA_USER` / `CAMERA_PASS` | Identifiants de la cam√©ra |
| `ELEVENLABS_API_KEY` | Pour la sortie vocale ‚Äî [elevenlabs.io](https://elevenlabs.io/) |
| `REALTIME_STT` | `true` pour activer la saisie vocale mains libres en temps r√©el (n√©cessite `ELEVENLABS_API_KEY`) |
| `TTS_OUTPUT` | Destination audio : `local` (haut-parleur PC, d√©faut) \| `remote` (haut-parleur cam√©ra) \| `both` (les deux simultan√©ment) |
| `THINKING_MODE` | Anthropic uniquement ‚Äî `auto` (d√©faut) \| `adaptive` \| `extended` \| `disabled` |
| `THINKING_EFFORT` | Profondeur de la r√©flexion adaptative : `high` (d√©faut) \| `medium` \| `low` \| `max` (Opus 4.6 uniquement) |

### 5. Cr√©er votre compagne

```bash
cp persona-template/en.md ME.md
# Modifiez ME.md ‚Äî donnez-lui un nom et une personnalit√©
```

### 6. Lancer

```bash
./run.sh             # Interface textuelle (recommand√©)
./run.sh --no-tui    # REPL simple
```

---

## Choisir un LLM

> **Recommand√© : Kimi K2.5** ‚Äî meilleure performance agentic test√©e jusqu'√† pr√©sent. Comprend le contexte, pose des questions de suivi et agit de mani√®re autonome d'une fa√ßon que d'autres mod√®les ne font pas. Prix comparable √† Claude Haiku.

| Plateforme | `PLATFORM=` | Mod√®le par d√©faut | O√π obtenir la cl√© |
|----------|------------|---------------|-----------------|
| **Moonshot Kimi K2.5** | `kimi` | `kimi-k2.5` | [platform.moonshot.ai](https://platform.moonshot.ai) |
| Z.AI GLM | `glm` | `glm-4.6v` | [api.z.ai](https://api.z.ai) |
| Anthropic Claude | `anthropic` | `claude-haiku-4-5-20251001` | [console.anthropic.com](https://console.anthropic.com) |
| Google Gemini | `gemini` | `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com) |
| OpenAI | `openai` | `gpt-4o-mini` | [platform.openai.com](https://platform.openai.com) |
| Compatible OpenAI (Ollama, vllm‚Ä¶) | `openai` + `BASE_URL=` | ‚Äî | ‚Äî |
| OpenRouter.ai (multi-fournisseurs) | `openai` + `BASE_URL=https://openrouter.ai/api/v1` | ‚Äî | [openrouter.ai](https://openrouter.ai) |
| **Outil CLI** (llm, ollama‚Ä¶) | `cli` | (la commande) | ‚Äî |

**Exemple `.env` pour Kimi K2.5 :**
```env
PLATFORM=kimi
API_KEY=sk-...   # from platform.moonshot.ai
AGENT_NAME=Yukine
```

**Exemple `.env` Z.AI GLM :**
```env
PLATFORM=glm
API_KEY=...   # from api.z.ai
MODEL=glm-4.6v   # vision-enabled; glm-4.7 / glm-5 = text-only
AGENT_NAME=Yukine
```

**Exemple `.env` pour Google Gemini :**
```env
PLATFORM=gemini
API_KEY=AIza...   # from aistudio.google.com
MODEL=gemini-2.5-flash  # or gemini-2.5-pro
AGENT_NAME=Yukine
```

**Exemple `.env` pour OpenRouter.ai :**
```env
PLATFORM=openai
BASE_URL=https://openrouter.ai/api/v1
API_KEY=sk-or-...   # from openrouter.ai
MODEL=mistralai/mistral-7b-instruct  # optional
AGENT_NAME=Yukine
```

> **Note :** Pour d√©sactiver les mod√®les locaux/NVIDIA, ne d√©finissez pas `BASE_URL` sur un endpoint local comme `http://localhost:11434/v1`. Utilisez plut√¥t des fournisseurs cloud.

**Exemple `.env` pour outil CLI :**
```env
PLATFORM=cli
MODEL=llm -m gemma3 {}        # llm CLI (https://llm.datasette.io) ‚Äî {} = arg du prompt
# MODEL=ollama run gemma3:27b  # Ollama ‚Äî sans {}, prompt via stdin
```

---

## Serveurs MCP

familiar-ai peut se connecter √† n'importe quel serveur [MCP (Model Context Protocol)](https://modelcontextprotocol.io) pour acc√©der √† la m√©moire externe, aux fichiers, √† la recherche web, etc.

Configurez les serveurs dans `~/.familiar-ai.json` (m√™me format que Claude Code) :

```json
{
  "mcpServers": {
    "filesystem": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user"]
    },
    "memory": {
      "type": "sse",
      "url": "http://localhost:3000/sse"
    }
  }
}
```

Deux types de transport sont support√©s :
- **`stdio`** : lance un sous-processus local (`command` + `args`)
- **`sse`** : se connecte √† un serveur HTTP+SSE (`url`)

Remplacez le chemin du fichier de config avec `MCP_CONFIG=/path/to/config.json`.

---

## Mat√©riel

familiar-ai fonctionne avec le mat√©riel que vous avez ‚Äî ou rien du tout.

| Composant | R√¥le | Exemple | Requis ? |
|------|-------------|---------|-----------|
| Cam√©ra PTZ Wi-Fi | Yeux + cou | Tapo C220 (~$30) | **Recommand√©** |
| Webcam USB | Yeux (fixes) | Toute cam√©ra UVC | **Recommand√©** |
| Aspirateur robot | Jambes | Tout mod√®le compatible Tuya | Non |
| PC / Raspberry Pi | Cerveau | Tout ce qui ex√©cute Python | **Oui** |

> **Une cam√©ra est fortement recommand√©e.** Sans elle, familiar-ai peut toujours parler ‚Äî mais elle ne peut pas voir le monde, ce qui est un peu tout l'int√©r√™t.

### Configuration minimale (sans mat√©riel)

Vous voulez juste l'essayer ? Vous n'avez besoin que d'une cl√© API :

```env
PLATFORM=kimi
API_KEY=sk-...
```

Lancez `./run.sh` et commencez √† discuter. Ajoutez du mat√©riel au fur et √† mesure.

### Cam√©ra PTZ Wi-Fi (Tapo C220)

1. Dans l'app Tapo : **Param√®tres ‚Üí Avanc√© ‚Üí Compte cam√©ra** ‚Äî cr√©ez un compte local (pas de compte TP-Link)
2. Trouvez l'IP de la cam√©ra dans la liste des appareils de votre routeur
3. D√©finissez dans `.env` :
   ```env
   CAMERA_HOST=192.168.1.xxx
   CAMERA_USER=your-local-user
   CAMERA_PASS=your-local-pass
   ```

### Voix (ElevenLabs)

1. Obtenez une cl√© API sur [elevenlabs.io](https://elevenlabs.io/)
2. D√©finissez dans `.env` :
   ```env
   ELEVENLABS_API_KEY=sk_...
   ELEVENLABS_VOICE_ID=...   # optionnel, utilise la voix par d√©faut si omis
   ```
La destination audio est contr√¥l√©e par `TTS_OUTPUT` :

```env
TTS_OUTPUT=local    # Haut-parleur PC (d√©faut)
TTS_OUTPUT=remote   # Haut-parleur cam√©ra uniquement
TTS_OUTPUT=both     # Haut-parleur cam√©ra + haut-parleur PC simultan√©ment
```

#### A) Haut-parleur de la cam√©ra (via go2rtc)

Utilisez `TTS_OUTPUT=remote` (ou `both`). Installez [go2rtc](https://github.com/AlexxIT/go2rtc/releases) manuellement :

1. T√©l√©chargez le binaire depuis la [page des releases](https://github.com/AlexxIT/go2rtc/releases) :
   - Linux/macOS : `go2rtc_linux_amd64` / `go2rtc_darwin_amd64`
   - **Windows : `go2rtc_win64.exe`**

2. Placez et renommez-le :
   ```
   # Linux / macOS
   ~/.cache/embodied-claude/go2rtc/go2rtc          # chmod +x requis

   # Windows
   %USERPROFILE%\.cache\embodied-claude\go2rtc\go2rtc.exe
   ```

3. Cr√©ez `go2rtc.yaml` dans le m√™me r√©pertoire :
   ```yaml
   streams:
     tapo_cam:
       - rtsp://YOUR_CAM_USER:YOUR_CAM_PASS@YOUR_CAM_IP/stream1
   ```

4. familiar-ai d√©marre go2rtc automatiquement. Si la cam√©ra supporte l'audio bidirectionnel, la voix sort du haut-parleur de la cam√©ra.

#### B) Haut-parleur PC local

Mode par d√©faut (`TTS_OUTPUT=local`). Essaie dans l'ordre : **paplay** ‚Üí **mpv** ‚Üí **ffplay**. √âgalement utilis√© en repli quand `TTS_OUTPUT=remote` et que go2rtc est indisponible.

| OS | Installation |
|----|-------------|
| macOS | `brew install mpv` |
| Ubuntu / Debian | `sudo apt install mpv` (ou `paplay` via `pulseaudio-utils`) |
| WSL2 / WSLg | `sudo apt install pulseaudio-utils` ‚Äî d√©finir `PULSE_SERVER=unix:/mnt/wslg/PulseServer` dans `.env` |
| Windows | [mpv.io/installation](https://mpv.io/installation/) ‚Äî t√©l√©charger et ajouter au PATH, **ou** `winget install ffmpeg` |

> Sans go2rtc ni lecteur local, la g√©n√©ration vocale (appel API ElevenLabs) fonctionne toujours ‚Äî la lecture est simplement ignor√©e.

### Saisie vocale (STT temps r√©el)

D√©finissez `REALTIME_STT=true` dans `.env` pour une saisie vocale mains libres en continu :

```env
REALTIME_STT=true
ELEVENLABS_API_KEY=sk_...   # m√™me cl√© que pour TTS
```

familiar-ai diffuse l'audio du microphone vers ElevenLabs Scribe v2 et valide les transcriptions automatiquement lorsque vous faites une pause. Aucune pression de touche requise. Compatible avec le mode push-to-talk (Ctrl+T).

---

## Interface textuelle

familiar-ai inclut une interface textuelle cr√©√©e avec [Textual](https://textual.textualize.io/) :

- Historique de conversation scrollable avec diffusion de texte en direct
- Compl√©ment de tabulation pour `/quit`, `/clear`
- Interrompez l'agent en cours d'ex√©cution en tapant pendant qu'il r√©fl√©chit
- **Journal de conversation** sauvegard√© automatiquement dans `~/.cache/familiar-ai/chat.log`

Pour suivre le journal dans un autre terminal (utile pour copier-coller) :
```bash
tail -f ~/.cache/familiar-ai/chat.log
```

---

## Persona (ME.md)

La personnalit√© de votre compagne vit dans `ME.md`. Ce fichier est ignor√© par git ‚Äî il vous appartient seul.

Consultez [`persona-template/en.md`](./persona-template/en.md) pour un exemple, ou [`persona-template/ja.md`](./persona-template/ja.md) pour une version japonaise.

---

## FAQ

**Q : √áa fonctionne sans GPU ?**
Oui. Le mod√®le d'embedding (multilingual-e5-small) s'ex√©cute bien sur CPU. Un GPU le rend plus rapide mais n'est pas requis.

**Q : Puis-je utiliser une cam√©ra autre que Tapo ?**
Toute cam√©ra supportant ONVIF + RTSP devrait fonctionner. Tapo C220 est ce que nous avons test√©.

**Q : Mes donn√©es sont-elles envoy√©es quelque part ?**
Les images et le texte sont envoy√©s √† l'API LLM de votre choix pour traitement. Les souvenirs sont stock√©s localement dans `~/.familiar_ai/`.

**Q : Pourquoi l'agent √©crit-il `Ôºà...Ôºâ` au lieu de parler ?**
Assurez-vous que `ELEVENLABS_API_KEY` est d√©fini. Sans lui, la voix est d√©sactiv√©e et l'agent revient au texte.

## Licence

[MIT](./LICENSE)
