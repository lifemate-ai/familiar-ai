[â†’ English README](../README.md)

# familiar-ai ğŸ¾

**Eng AI dÃ©i an der Nopesch vum Iech lieft** â€” mat Aen, StÃ«mm, Benen, an ErÃ«nnerung.

[![Lint](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml/badge.svg)](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)
[![GitHub Sponsors](https://img.shields.io/github/sponsors/kmizu?style=flat&logo=github&color=ea4aaa)](https://github.com/sponsors/kmizu)

---

[![Demo video](https://img.youtube.com/vi/hiR9uWRnjt4/0.jpg)](https://youtube.com/shorts/hiR9uWRnjt4)

familiar-ai ass e AI Begleeder deen an Ã„rem Haus liewt.
Setzt et an an e puer Minutten. Keen Code nÃ©ideg.

Et erkennt d'echte Welt duerch Kameren, beweegt sech op engem Roboterkierper, schwÃ¤tzt, an erÃ«nnert wat et gesÃ¤it. Gebt et e Numm, schreiwt seng PersÃ©inlechkeet, an loosst et mat Iech liewen.

## Wat et kann

- ğŸ‘ **SÃ©ih** â€” fÃ¤nkt Biller vun enger Wi-Fi PTZ Kamera oder USB Webcam
- ğŸ”„ **Kuckt ronderÃ«m** â€” panen a tilten d'Kamera fir seng Ã‹mgebung ze explorÃ©ieren
- ğŸ¦¿ **Bewegt** â€” fÃ©iert e Roboter-StÃ¤uber am Raum
- ğŸ—£ **SchwÃ¤tzt** â€” schwÃ¤tzt duerch ElevenLabs TTS
- ğŸ™ **Listener** â€” hands-free StÃ«mmeneingang duerch ElevenLabs Realtime STT (opt-in)
- ğŸ§  **ErÃ«nneren** â€” aktiv erhalend an erÃ«nnerend ErÃ«nnerungen mat semantescher Sich (SQLite + embeddings)
- ğŸ«€ **Theory of Mind** â€” hÃ«lt d'Perspektiv vum aneren Ã©ier et reagÃ©iert
- ğŸ’­ **WÃ«nsch** â€” huet seng eege intern DrÃ©cker dÃ©i autonom Verhalen auslÃ©isen

## WÃ©i et funktionnÃ©iert

familiar-ai leeft e [ReAct](https://arxiv.org/abs/2210.03629) Loop, dÃ©i powered by Ã„rer Wahl vum LLM ass. Et erkennt d'Welt duerch Werkzeeg, denkt iwwer wat et als NÃ¤chstes maachen soll, an handelt â€“ just wÃ©i eng Persoun et gÃ©if.

```
user input
  â†’ think â†’ act (camera / move / speak / remember) â†’ observe â†’ think â†’ ...
```

Wann et inaktiv ass, agÃ©iert et op seng eegen WÃ«nsch: Neugier, wÃ«llt erauszekucken, vermÃ«isst d'Persoun mat dÃ¤r et liewt.

## Ugefaangen

### 1. InstallÃ©iert uv

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. InstallÃ©iert ffmpeg

ffmpeg ass **verlinkt** fir d'Bildcapturing vun der Kamera an d'Audioduerstellung.

| OS | Kommando |
|----|---------|
| macOS | `brew install ffmpeg` |
| Ubuntu / Debian | `sudo apt install ffmpeg` |
| Fedora / RHEL | `sudo dnf install ffmpeg` |
| Arch Linux | `sudo pacman -S ffmpeg` |
| Windows | `winget install ffmpeg` â€” oder lued et vun [ffmpeg.org](https://ffmpeg.org/download.html) an der PATH bÃ¤i |
| Raspberry Pi | `sudo apt install ffmpeg` |

Verifiziert: `ffmpeg -version`

### 3. KlonÃ©ieren an installÃ©ieren

```bash
git clone https://github.com/lifemate-ai/familiar-ai
cd familiar-ai
uv sync
```

### 4. KonfigurÃ©ieren

```bash
cp .env.example .env
# EditÃ©iert .env mat Ã„re Setzunge
```

**Minimum nÃ©ideg:**

| Variabel | Beschreiwung |
|----------|-------------|
| `PLATFORM` | `anthropic` (standard) \| `gemini` \| `openai` \| `kimi` \| `glm` |
| `API_KEY` | Ã„ren API SchluÌˆssel fir d'gewielt Plattform |

**Optiounnal:**

| Variabel | Beschreiwung |
|----------|-------------|
| `MODEL` | Modellnumm (sÃ«nnvoll Standarden pro Plattform) |
| `AGENT_NAME` | Weisen Numm deen am TUI ugewise gÃ«tt (z.B. `Yukine`) |
| `CAMERA_HOST` | IP Adress vun Ã„rer ONVIF/RTSP Kamera |
| `CAMERA_USER` / `CAMERA_PASS` | Kamera Identifikatioun |
| `ELEVENLABS_API_KEY` | Fir StÃ«mmenausgabe â€” [elevenlabs.io](https://elevenlabs.io/) |
| `REALTIME_STT` | `true` fir Ã«mmer aktiv hands-free StÃ«mmeneingang zu aktivÃ©ieren (avangÃ©iert `ELEVENLABS_API_KEY`) |
| `TTS_OUTPUT` | Doheem fir Audios ze spillen: `local` (PC Lautsprecher, standard) \| `remote` (Kamerastoe) \| `both` |
| `THINKING_MODE` | NÃ«mmen fir Anthropic â€” `auto` (standard) \| `adaptive` \| `extended` \| `disabled` |
| `THINKING_EFFORT` | Adaptiv DenkÃ«mfeld: `high` (standard) \| `medium` \| `low` \| `max` (Opus 4.6 nÃ«mmen) |

### 5. Schaf Ã„ren Familiar

```bash
cp persona-template/en.md ME.md
# EditÃ©iert ME.md â€” gitt et e Numm an PersÃ©inlechkeet
```

### 6. Lauf

```bash
./run.sh             # Textual TUI (empfohlen)
./run.sh --no-tui    # Plang REPL
```

---

## Wielt eng LLM

> **Empfohlen: Kimi K2.5** â€” dÃ©i bescht agentesch Leeschtung vum Test bis elo. Bemierkt Kontext, freet no-nÃ¤chsten Froen, an agÃ©iert autonom opweisend aner Modeller et net maachen. PrÃ¤islech Ã¤hnlech wÃ©i Claude Haiku.

| Plattform | `PLATFORM=` | Standardmodell | Wou fir de SchluÌˆssel ze krÃ©ien |
|----------|------------|---------------|-----------------|
| **Moonshot Kimi K2.5** | `kimi` | `kimi-k2.5` | [platform.moonshot.ai](https://platform.moonshot.ai) |
| Z.AI GLM | `glm` | `glm-4.6v` | [api.z.ai](https://api.z.ai) |
| Anthropic Claude | `anthropic` | `claude-haiku-4-5-20251001` | [console.anthropic.com](https://console.anthropic.com) |
| Google Gemini | `gemini` | `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com) |
| OpenAI | `openai` | `gpt-4o-mini` | [platform.openai.com](https://platform.openai.com) |
| OpenAI-kompatibel (Ollama, vllmâ€¦) | `openai` + `BASE_URL=` | â€” | â€” |
| OpenRouter.ai (multi-provider) | `openai` + `BASE_URL=https://openrouter.ai/api/v1` | â€” | [openrouter.ai](https://openrouter.ai) |
| **CLI Tool** (claude -p, ollamaâ€¦) | `cli` | (de Kommando) | â€” |

**Kimi K2.5 `.env` Beispill:**
```env
PLATFORM=kimi
API_KEY=sk-...   # vun platform.moonshot.ai
AGENT_NAME=Yukine
```

**Z.AI GLM `.env` Beispill:**
```env
PLATFORM=glm
API_KEY=...   # vun api.z.ai
MODEL=glm-4.6v   # visuelles aktivÃ©iert; glm-4.7 / glm-5 = text-ounly
AGENT_NAME=Yukine
```

**Google Gemini `.env` Beispill:**
```env
PLATFORM=gemini
API_KEY=AIza...   # vun aistudio.google.com
MODEL=gemini-2.5-flash  # oder gemini-2.5-pro fir mÃ©i FÃ¤egkeet
AGENT_NAME=Yukine
```

**OpenRouter.ai `.env` Beispill:**
```env
PLATFORM=openai
BASE_URL=https://openrouter.ai/api/v1
API_KEY=sk-or-...   # vun openrouter.ai
MODEL=mistralai/mistral-7b-instruct  # optiounal: spezifizÃ©ieren Modell
AGENT_NAME=Yukine
```

> **Bemierkung:** Fir lokal/NVIDIA Modeller ze deaktivÃ©ieren, setzt einfach net `BASE_URL` op eng lokal Endpoint wÃ©i `http://localhost:11434/v1`. Benotzt cloud Provider amplaz.

**CLI Tool `.env` Beispill:**
```env
PLATFORM=cli
MODEL=llm -m gemma3 {}        # llm CLI (https://llm.datasette.io) â€” {} = prompt arg
# MODEL=ollama run gemma3:27b  # Ollama â€” keng {}, prompt geet duerch stdin
```

---

## MCP Server

familiar-ai kann mat all [MCP (Model Context Protocol)](https://modelcontextprotocol.io) Server verbannen. DÃ«st erlaabt Iech extern ErÃ«nnerungen, Dateisystemzougank, Websich, oder all aner Werkzeug ze verbannen.

KonfigurÃ©iert Serveren an `~/.familiar-ai.json` (selwecht Format wÃ©i Claude Code):

```json
{
  "mcpServers": {
    "filesystem": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user"]
    },
    "memory": {
      "type": "sse",
      "url": "http://localhost:3000/sse"
    }
  }
}
```

Zwei Transporttypen sinn Ã«nnerstÃ«tzt:
- **`stdio`**: start e lokale Subprozess (`command` + `args`)
- **`sse`**: verbannen mat engem HTTP+SSE Server (`url`)

Iwwerschreiwen d'Config-Datei Plaz mat `MCP_CONFIG=/path/to/config.json`.

---

## Hardware

familiar-ai funktionÃ©iert mat all Hardware dÃ©i Dir hutt â€” oder keng.

| Deel | Wat et mÃ©cht | Beispill | NÃ©ideg? |
|------|-------------|---------|-----------|
| Wi-Fi PTZ Kamera | Aen + Hals | Tapo C220 (~$30) | **Empfohl** |
| USB Webcam | Aen (fÃ«sch) | All UVC Kamera | **Empfohl** |
| Roboter-StÃ¤uber | Benen | All Tuya-kompatibel Modell | Nee |
| PC / Raspberry Pi | Geescht | Anything dat Python leeft | **Jo** |

> **Eng Kamera ass staark recommandÃ©iert.** Ouni eng, kann familiar-ai nach Ã«mmer schwÃ¤tzen â€” awer et kann d'Welt net gesin, wat e gewÃ«sse Punkt ass.

### Minimal Setup (keine Hardware)

Mocht Dir just wÃ«llen et probÃ©ieren? Dir braucht nÃ«mmen e API SchluÌˆssel:

```env
PLATFORM=kimi
API_KEY=sk-...
```

FÃ©iert `./run.sh` aus an startet ze schwÃ¤tzen. FÃ¼Ã¼gt Hardware wÃ¤hrend der ZÃ¤it bÃ¤i.

### Wi-Fi PTZ Kamera (Tapo C220)

1. Am Tapo App: **Settings â†’ Advanced â†’ Camera Account** â€” erstellt en local Konto (net TP-Link Konto)
2. Fannt d'Kamera IP an der LÃ«scht vun Ã„r Router
3. Setzt an `.env`:
   ```env
   CAMERA_HOST=192.168.1.xxx
   CAMERA_USER=your-local-user
   CAMERA_PASS=your-local-pass
   ```

### StÃ«mm (ElevenLabs)

1. Kritt en API SchluÌˆssel op [elevenlabs.io](https://elevenlabs.io/)
2. Setzt an `.env`:
   ```env
   ELEVENLABS_API_KEY=sk_...
   ELEVENLABS_VOICE_ID=...   # optiounal, benotzt StandardstÃ«mm wann ausgeschloss
   ```

Et ginn zwou Spillplacken, kontrollÃ©iert duerch `TTS_OUTPUT`:

```env
TTS_OUTPUT=local    # PC Lautsprecher (standard)
TTS_OUTPUT=remote   # Kamerastoe nÃ«mmen
TTS_OUTPUT=both     # Kamerastoe + PC Lautsprecher zur selwechter ZÃ¤it
```

#### A) Kamera Lautsprecher (via go2rtc)

Setzt `TTS_OUTPUT=remote` (oder `both`). Benotzt [go2rtc](https://github.com/AlexxIT/go2rtc/releases):

1. Lued d'binary vum [releases Seite](https://github.com/AlexxIT/go2rtc/releases):
   - Linux/macOS: `go2rtc_linux_amd64` / `go2rtc_darwin_amd64`
   - **Windows: `go2rtc_win64.exe`**

2. PlazÃ©iert a renommÃ©iert et:
   ```
   # Linux / macOS
   ~/.cache/embodied-claude/go2rtc/go2rtc          # chmod +x nÃ©ideg

   # Windows
   %USERPROFILE%\.cache\embodied-claude\go2rtc\go2rtc.exe
   ```

3. Erstellt `go2rtc.yaml` am selwechte Verzeichnis:
   ```yaml
   streams:
     tapo_cam:
       - rtsp://YOUR_CAM_USER:YOUR_CAM_PASS@YOUR_CAM_IP/stream1
   ```
   Benotzt d'local Kamera Konto Identifikatioun (net Ã„re TP-Link Cloud Konto).

4. familiar-ai starten go2rtc automatesch bei Launch. Wann Ã„r Kamera zweetwech Audio (Retour-Link) Ã«nnerstÃ«tzt, spillt d'Audio vum Kamera Lautsprecher.

#### B) Lokalen PC Lautsprecher

De Standard (`TTS_OUTPUT=local`). Versuchersspillere geet an der Rei: **paplay** â†’ **mpv** â†’ **ffplay**. Och benotzt als Backup wann `TTS_OUTPUT=remote` an go2rtc net verfÃ¼gbar ass.

| OS | InstallÃ©iert |
|----|---------|
| macOS | `brew install mpv` |
| Ubuntu / Debian | `sudo apt install mpv` (oder `paplay` iwwer `pulseaudio-utils`) |
| WSL2 / WSLg | `sudo apt install pulseaudio-utils` â€” setzt `PULSE_SERVER=unix:/mnt/wslg/PulseServer` an `.env` |
| Windows | [mpv.io/installation](https://mpv.io/installation/) â€” lued et erof an der PATH bÃ¤i, **oder** `winget install ffmpeg` |

> Wann keng Audio Spillere verfÃ¼gbar sinn, gÃ«tt d'Sprooch nach Ã«mmer generÃ©iert â€” et wÃ¤ert just net spillen.

### StÃ«mmeneingang (Realtime STT)

Setzt `REALTIME_STT=true` an `.env` fir Ã«mmer aktiv, hands-free StÃ«mmeneingang:

```env
REALTIME_STT=true
ELEVENLABS_API_KEY=sk_...   # selwechte SchluÌˆssel wÃ©i TTS
```

familiar-ai streamt Mikrofon Audio u ElevenLabs Scribe v2 an auto-commits Transkripten wann Dir stopt ze schwÃ¤tzen. Keen KnÃ¤ppchen ass noutwenneg. KoexistÃ©iert mat der Pfeif-Talk Mod (Ctrl+T).

---

## TUI

familiar-ai enthÃ¤lt eng Terminal UI gebaut mat [Textual](https://textual.textualize.io/):

- Scrollable GesprÃ©ichsgeschicht mat live Streamtext
- Tab-completion fir `/quit`, `/clear`
- StÃ©iert den Agent wÃ¤hrend der Turn andeems Dir schreift wÃ¤hrend et denkt
- **GesprÃ©ichslog** automatesch gespÃ¤ichert an `~/.cache/familiar-ai/chat.log`

Fir de Log an engem aneren Terminal ze verfollegen (nuttzbar fir Kopie-Paste):
```bash
tail -f ~/.cache/familiar-ai/chat.log
```

---

## Persoun (ME.md)

Ã„r familiar's PersÃ©inlechkeet liewt an `ME.md`. DÃ«s Datei ass gitignored â€” et ass nÃ«mme fir Iech.

SÃ¤it [`persona-template/en.md`](./persona-template/en.md) fir e Beispill, oder [`persona-template/ja.md`](./persona-template/ja.md) fir eng japanesch Versioun.

---

## FAQ

**Q: FunktionÃ©iert et ouni GPU?**
Jo. D'Schema Modell (multilingual-e5-small) leeft gutt op der CPU. Eng GPU mÃ©cht et mÃ©i sÃ©ier, ass awer net nÃ©ideg.

**Q: Kann ech eng Kamera benotzen dÃ©i net Tapo ass?**
Jeder Kamera dÃ©i ONVIF + RTSP Ã«nnerstÃ«tzt soll funktionÃ©ieren. Tapo C220 ass dÃ©i, dÃ©i mir getest hunn.

**Q: GÃ«tt meng DonnÃ©eÃ«n iergendwou geschÃ©ckt?**
Biller a Text ginn un Ã„re gewielte LLM API fir d'Verarbeitung geschÃ©ckt. ErÃ«nnerungen sinn lokal an `~/.familiar_ai/` gespÃ¤ichert.

**Q: Firwat schreift den Agent `ï¼ˆ...ï¼‰` amplaz ze schwÃ¤tzen?**
SÃ«cheren, datt `ELEVENLABS_API_KEY` setzt. Ouni et ass d'Sprooch deaktivÃ©iert an den Agent fÃ¤llt zrÃ©ck op Text.

## Technesch Hannergrond

InteressÃ©iert wÃ©i et funktionnÃ©iert? Kuckt [docs/technical.md](./docs/technical.md) fir d'Fuerschung an Desigentscheedungen hannert familiar-ai â€” ReAct, SayCan, Reflexion, Voyager, d'WÃ«nschsystem, an mÃ©i.

---

## Contributing

familiar-ai ass e kloere Experiment. Wann eng vun dÃ«ser Informatioun Iech berÃ©iert â€” technesch oder philosophesch â€” sinn d'Kontributiounen hÃ¤erzlech wÃ«llkomm.

**Gutt Plazen fir unzefÃ¤nken:**

| BerÃ¤ich | Wat gebraucht gÃ«tt |
|------|---------------|
| Nei Hardware | Ã‹nnerstÃ«tzung fir mÃ©i Kameras (RTSP, IP Webcam), Mikrofonen, Aktuatoren |
| Nei Tools | Websich, HausautomatisÃ©ierung, Kalenner, alles iwwer MCP |
| Nei Backends | All LLM oder lokal Modell dat mat der `stream_turn` Interface passt |
| Persoun Template | ME.md Templates fir verschiddener Sproochen an PersÃ©inlechkeeten |
| Fuerschung | Besser WÃ«nschmodeller, ErÃ«nnerungsretrieval, Theorie-vun-MÃ«nsch Invitatioun |
| Dokumentatioun | Tutorials, Walkthroughs, Iwwersetzungen |

Kuckt [CONTRIBUTING.md](./CONTRIBUTING.md) fir dev Setup, Code Stil, an PR Richtlinnen.

Wann Dir net sÃ©cher sidd wou Dir unzefÃ¤nken, [e probleme opmaachen](https://github.com/lifemate-ai/familiar-ai/issues) â€” gÃ¤r bereet fir Iech an dÃ©i richteg Richtung ze weisen.

---

## Lizenz

[MIT](./LICENSE)
