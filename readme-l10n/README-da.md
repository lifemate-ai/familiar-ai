# familiar-ai üêæ

**En AI, der lever sammen med dig** ‚Äî med √∏jne, stemme, ben og hukommelse.

[![Lint](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml/badge.svg)](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)
[![GitHub Sponsors](https://img.shields.io/github/sponsors/kmizu?style=flat&logo=github&color=ea4aaa)](https://github.com/sponsors/kmizu)

üåç [Tilg√¶ngelig p√• 74 sprog](./SUPPORTED_LANGUAGES.md)

---

[![Demo video](https://img.youtube.com/vi/hiR9uWRnjt4/0.jpg)](https://youtube.com/shorts/hiR9uWRnjt4)

familiar-ai er en AI-ven, der lever i dit hjem. 
S√¶t det op p√• f√• minutter. Ingen kodning kr√¶vet.

Det opfatter den virkelige verden gennem kameraer, bev√¶ger sig rundt p√• en robotkrop, taler h√∏jt og husker, hvad det ser. Giv det et navn, skriv dens personlighed, og lad det bo sammen med dig.

## Hvad det kan g√∏re

- üëÅ **Se** ‚Äî fanger billeder fra et Wi-Fi PTZ-kamera eller USB-webcam
- üîÑ **Se rundt** ‚Äî panorerer og tilter kameraet for at udforske omgivelserne
- ü¶ø **Bev√¶ge sig** ‚Äî k√∏rer en robotst√∏vsuger for at f√¶rdes i rummet
- üó£ **Tale** ‚Äî taler via ElevenLabs TTS
- üéô **Lytte** ‚Äî h√•ndfri stemmeinddata via ElevenLabs Realtime STT (opt-in)
- üß† **Huske** ‚Äî gemmer og husker aktivt minder med semantisk s√∏gning (SQLite + embeddings)
- ü´Ä **Teori om sind** ‚Äî tager den andens perspektiv, f√∏r den svarer
- üí≠ **√ònske** ‚Äî har sine egne indre drifter, der udl√∏ser autonom adf√¶rd

## Hvordan det fungerer

familiar-ai k√∏rer en [ReAct](https://arxiv.org/abs/2210.03629) l√∏kke drevet af dit valg af LLM. Den opfatter verden gennem v√¶rkt√∏jer, t√¶nker over, hvad der skal g√∏res n√¶ste gang, og handler ‚Äî ligesom en person ville.

```
user input
  ‚Üí think ‚Üí act (camera / move / speak / remember) ‚Üí observe ‚Üí think ‚Üí ...
```

N√•r den er inaktiv, handler den p√• sine egne √∏nsker: nysgerrighed, √∏nsker at se udenfor, savner personen, den lever sammen med.

## Kom godt i gang

### 1. Installer uv

**macOS / Linux / WSL2:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Windows (PowerShell):**
```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```
Eller: `winget install astral-sh.uv`

### 2. Installer ffmpeg

ffmpeg er **p√•kr√¶vet** for kamera billedefangst og lydafspilning.

| OS | Kommando |
|----|---------|
| macOS | `brew install ffmpeg` |
| Ubuntu / Debian | `sudo apt install ffmpeg` |
| Fedora / RHEL | `sudo dnf install ffmpeg` |
| Arch Linux | `sudo pacman -S ffmpeg` |
| Windows | `winget install ffmpeg` ‚Äî eller download fra [ffmpeg.org](https://ffmpeg.org/download.html) og tilf√∏j til PATH |
| Raspberry Pi | `sudo apt install ffmpeg` |

Bekr√¶ft: `ffmpeg -version`

### 3. Klon og installer

```bash
git clone https://github.com/lifemate-ai/familiar-ai
cd familiar-ai
uv sync
```

### 4. Konfigurere

```bash
cp .env.example .env
# Rediger .env med dine indstillinger
```

**Minimum p√•kr√¶vet:**

| Variabel | Beskrivelse |
|----------|-------------|
| `PLATFORM` | `anthropic` (standard) \| `gemini` \| `openai` \| `kimi` \| `glm` |
| `API_KEY` | Din API-n√∏gle for den valgte platform |

**Valgfrit:**

| Variabel | Beskrivelse |
|----------|-------------|
| `MODEL` | Modelnavn (fornuftige standardindstillinger pr. platform) |
| `AGENT_NAME` | Visningsnavn vist i TUI (f.eks. `Yukine`) |
| `CAMERA_HOST` | IP-adresse for dit ONVIF/RTSP kamera |
| `CAMERA_USER` / `CAMERA_PASS` | Kameraoplysninger |
| `ELEVENLABS_API_KEY` | For stemmeoutput ‚Äî [elevenlabs.io](https://elevenlabs.io/) |
| `REALTIME_STT` | `true` for at aktivere altid-t√¶ndt h√•ndfri stemmeinddata (kr√¶ver `ELEVENLABS_API_KEY`) |
| `TTS_OUTPUT` | Hvor lyd skal afspilles: `local` (PC-h√∏jttaler, standard) \| `remote` (kamera-h√∏jttaler) \| `both` |
| `THINKING_MODE` | Kun Anthropic ‚Äî `auto` (standard) \| `adaptive` \| `extended` \| `disabled` |
| `THINKING_EFFORT` | Adaptiv t√¶nkeindsats: `high` (standard) \| `medium` \| `low` \| `max` (kun Opus 4.6) |

### 5. Opret din familiar

```bash
cp persona-template/en.md ME.md
# Rediger ME.md ‚Äî giv det et navn og personlighed
```

### 6. K√∏r

**macOS / Linux / WSL2:**
```bash
./run.sh             # Tekstuel TUI (anbefales)
./run.sh --no-tui    # Simpel REPL
```

**Windows:**
```bat
run.bat              # Tekstuel TUI (anbefales)
run.bat --no-tui     # Simpel REPL
```

---

## V√¶lge en LLM

> **Anbefalet: Kimi K2.5** ‚Äî bedst agentisk pr√¶station testet hidtil. L√¶gger m√¶rke til konteksten, stiller opf√∏lgende sp√∏rgsm√•l, og handler autonomt p√• m√•der, som andre modeller ikke g√∏r. Prissat tilsvarende Claude Haiku.

| Platform | `PLATFORM=` | Standardmodel | Hvor man kan f√• n√∏gle |
|----------|------------|---------------|-----------------|
| **Moonshot Kimi K2.5** | `kimi` | `kimi-k2.5` | [platform.moonshot.ai](https://platform.moonshot.ai) |
| Z.AI GLM | `glm` | `glm-4.6v` | [api.z.ai](https://api.z.ai) |
| Anthropic Claude | `anthropic` | `claude-haiku-4-5-20251001` | [console.anthropic.com](https://console.anthropic.com) |
| Google Gemini | `gemini` | `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com) |
| OpenAI | `openai` | `gpt-4o-mini` | [platform.openai.com](https://platform.openai.com) |
| OpenAI-kompatibel (Ollama, vllm‚Ä¶) | `openai` + `BASE_URL=` | ‚Äî | ‚Äî |
| OpenRouter.ai (multi-provider) | `openai` + `BASE_URL=https://openrouter.ai/api/v1` | ‚Äî | [openrouter.ai](https://openrouter.ai) |
| **CLI-v√¶rkt√∏j** (claude -p, ollama‚Ä¶) | `cli` | (kommandoen) | ‚Äî |

**Kimi K2.5 `.env` eksempel:**
```env
PLATFORM=kimi
API_KEY=sk-...   # fra platform.moonshot.ai
AGENT_NAME=Yukine
```

**Z.AI GLM `.env` eksempel:**
```env
PLATFORM=glm
API_KEY=...   # fra api.z.ai
MODEL=glm-4.6v   # vision-aktiveret; glm-4.7 / glm-5 = tekstkun
AGENT_NAME=Yukine
```

**Google Gemini `.env` eksempel:**
```env
PLATFORM=gemini
API_KEY=AIza...   # fra aistudio.google.com
MODEL=gemini-2.5-flash  # eller gemini-2.5-pro for h√∏jere kapabilitet
AGENT_NAME=Yukine
```

**OpenRouter.ai `.env` eksempel:**
```env
PLATFORM=openai
BASE_URL=https://openrouter.ai/api/v1
API_KEY=sk-or-...   # fra openrouter.ai
MODEL=mistralai/mistral-7b-instruct  # valgfrit: specificer model
AGENT_NAME=Yukine
```

> **Bem√¶rk:** For at deaktivere lokale/NVIDIA-modeller skal du simpelt set ikke indstille `BASE_URL` til en lokal slutpunkt som `http://localhost:11434/v1`. Brug i stedet cloud-udbydere.

**CLI-v√¶rkt√∏j `.env` eksempel:**
```env
PLATFORM=cli
MODEL=llm -m gemma3 {}        # llm CLI (https://llm.datasette.io) ‚Äî {} = prompt-argument
# MODEL=ollama run gemma3:27b  # Ollama ‚Äî ingen {}, prompt g√•r via stdin
```

---

## MCP Servere

familiar-ai kan forbinde til enhver [MCP (Model Context Protocol)](https://modelcontextprotocol.io) server. Dette lader dig tilslutte ekstern hukommelse, filsystemadgang, webs√∏gnings, eller ethvert andet v√¶rkt√∏j.

Konfigurer servere i `~/.familiar-ai.json` (samme format som Claude Code):

```json
{
  "mcpServers": {
    "filesystem": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user"]
    },
    "memory": {
      "type": "sse",
      "url": "http://localhost:3000/sse"
    }
  }
}
```

To transporttyper st√∏ttes:
- **`stdio`**: lancerer en lokal underproces (`command` + `args`)
- **`sse`**: forbinder til en HTTP+SSE server (`url`)

Overskriv konfigurationsfilens placering med `MCP_CONFIG=/path/to/config.json`.

---

## Hardware

familiar-ai fungerer med hvilket som helst hardware, du har ‚Äî eller slet ingen.

| Del | Hvad den g√∏r | Eksempel | Required? |
|------|-------------|---------|-----------|
| Wi-Fi PTZ kamera | √òjne + nakke | Tapo C220 (~$30) | **Anbefales** |
| USB webcam | √òjne (fast) | Ethvert UVC kamera | **Anbefales** |
| Robotst√∏vsuger | Ben | Enhver Tuya-kompatibel model | Nej |
| PC / Raspberry Pi | Hjerne | Alt, der k√∏rer Python | **Ja** |

> **Et kamera anbefales st√¶rkt.** Uden et, kan familiar-ai stadig tale ‚Äî men den kan ikke se verden, hvilket er lidt af hele pointen.

### Minimal ops√¶tning (ingen hardware)

Vil du bare pr√∏ve det? Du har kun brug for en API-n√∏gle:

```env
PLATFORM=kimi
API_KEY=sk-...
```

K√∏r `./run.sh` (macOS/Linux/WSL2) eller `run.bat` (Windows) og begynd at chatte. Tilf√∏j hardware, som du g√•r.

### Wi-Fi PTZ kamera (Tapo C220)

1. I Tapo-appen: **Indstillinger ‚Üí Avanceret ‚Üí Kamera Konto** ‚Äî opret en lokal konto (ikke TP-Link konto)
2. Find kameraets IP i din routers enhedsliste
3. Indstil i `.env`:
   ```env
   CAMERA_HOST=192.168.1.xxx
   CAMERA_USER=your-local-user
   CAMERA_PASS=your-local-pass
   ```

### Stemme (ElevenLabs)

1. F√• en API-n√∏gle p√• [elevenlabs.io](https://elevenlabs.io/)
2. Indstil i `.env`:
   ```env
   ELEVENLABS_API_KEY=sk_...
   ELEVENLABS_VOICE_ID=...   # valgfrit, bruger standardstemmen hvis udeladt
   ```

Der er to afspilningsdestinationer, styret af `TTS_OUTPUT`:

```env
TTS_OUTPUT=local    # PC-h√∏jttaler (standard)
TTS_OUTPUT=remote   # kamera-h√∏jttaler kun
TTS_OUTPUT=both     # kamera-h√∏jttaler + PC-h√∏jttaler samtidigt
```

#### A) Kamera h√∏jttaler (via go2rtc)

Indstil `TTS_OUTPUT=remote` (eller `both`). Kr√¶ver [go2rtc](https://github.com/AlexxIT/go2rtc/releases):

1. Download bin√¶ren fra [udgivelsessiden](https://github.com/AlexxIT/go2rtc/releases):
   - Linux/macOS: `go2rtc_linux_amd64` / `go2rtc_darwin_amd64`
   - **Windows: `go2rtc_win64.exe`**

2. Placer og omd√∏b den:
   ```
   # Linux / macOS
   ~/.cache/embodied-claude/go2rtc/go2rtc          # chmod +x p√•kr√¶vet

   # Windows
   %USERPROFILE%\.cache\embodied-claude\go2rtc\go2rtc.exe
   ```

3. Opret `go2rtc.yaml` i samme mappe:
   ```yaml
   streams:
     tapo_cam:
       - rtsp://YOUR_CAM_USER:YOUR_CAM_PASS@YOUR_CAM_IP/stream1
   ```
   Brug de lokale kameraoplysninger (ikke din TP-Link cloud-konto).

4. familiar-ai starter go2rtc automatisk ved opstart. Hvis dit kamera underst√∏tter tovejs lyd (backchannel), spiller stemmen fra kameraets h√∏jttaler.

#### B) Lokal PC-h√∏jttaler

Den standard (`TTS_OUTPUT=local`). Fors√∏ger spillere i r√¶kkef√∏lge: **paplay** ‚Üí **mpv** ‚Üí **ffplay**. Ogs√• brugt som en fallback n√•r `TTS_OUTPUT=remote` og go2rtc ikke er tilg√¶ngelig.

| OS | Installer |
|----|---------|
| macOS | `brew install mpv` |
| Ubuntu / Debian | `sudo apt install mpv` (eller `paplay` via `pulseaudio-utils`) |
| WSL2 / WSLg | `sudo apt install pulseaudio-utils` ‚Äî indstil `PULSE_SERVER=unix:/mnt/wslg/PulseServer` i `.env` |
| Windows | [mpv.io/installation](https://mpv.io/installation/) ‚Äî download og tilf√∏j til PATH, **eller** `winget install ffmpeg` |

> Hvis ingen lydafspiller er tilg√¶ngelig, genereres stadig tale ‚Äî den vil bare ikke afspille.

### Stemmeinddata (Realtime STT)

Indstil `REALTIME_STT=true` i `.env` for altid-t√¶ndt, h√•ndfri stemmeinddata:

```env
REALTIME_STT=true
ELEVENLABS_API_KEY=sk_...   # samme n√∏gle som TTS
```

familiar-ai streamer mikrofonlyd til ElevenLabs Scribe v2 og auto-kommitter transkripter, n√•r du holder pause med at tale. Ingen knaptryk kr√¶ves. Sameksisterer med tryk-for-at-tale tilstand (Ctrl+T).

---

## TUI

familiar-ai inkluderer en terminal UI bygget med [Textual](https://textual.textualize.io/):

- Scrollerbar samtalehistorik med live streaming tekst
- Tabfuldf√∏relse for `/quit`, `/clear`
- Afbryd agenten midt i en tur ved at skrive, mens den t√¶nker
- **Samtalelog** auto-gemmes til `~/.cache/familiar-ai/chat.log`

For at f√∏lge loggen i en anden terminal (nyttigt til kopi-inds√¶t):
```bash
tail -f ~/.cache/familiar-ai/chat.log
```

---

## Persona (ME.md)

Din familiars personlighed lever i `ME.md`. Denne fil er gitignored ‚Äî det er kun din.

Se [`persona-template/en.md`](./persona-template/en.md) for et eksempel, eller [`persona-template/ja.md`](./persona-template/ja.md) for en japansk version.

---

## FAQ

**Q: Fungerer det uden en GPU?**
Ja. Embedding-modellen (multilingual-e5-small) k√∏rer fint p√• CPU. En GPU g√∏r det hurtigere, men den er ikke p√•kr√¶vet.

**Q: Kan jeg bruge et kamera, der ikke er Tapo?**
Ethvert kamera, der underst√∏tter ONVIF + RTSP, b√∏r fungere. Tapo C220 er hvad vi har testet med.

**Q: Bliver mine data sendt nogen steder?**
Billeder og tekst sendes til din valgte LLM API til behandling. Minder gemmes lokalt i `~/.familiar_ai/`.

**Q: Hvorfor skriver agenten `Ôºà...Ôºâ` i stedet for at tale?**
S√∏rg for, at `ELEVENLABS_API_KEY` er indstillet. Uden det, er stemmen deaktiveret, og agenten falder tilbage til tekst.

## Teknisk baggrund

Nysgerrig p√• hvordan det fungerer? Se [docs/technical.md](./docs/technical.md) for forskningen og designbeslutningerne bag familiar-ai ‚Äî ReAct, SayCan, Reflexion, Voyager, desire-systemet og mere.

---

## Bidrag

familiar-ai er et √•bent eksperiment. Hvis noget af dette resonerer med dig ‚Äî teknisk eller filosofisk ‚Äî bidrag er meget velkomne.

**Gode steder at starte:**

| Omr√•de | Hvad der er n√∏dvendigt |
|------|---------------|
| Nyt hardware | Support til flere kameraer (RTSP, IP Webcam), mikrofoner, aktuatorer |
| Nye v√¶rkt√∏jer | Webs√∏gnings, hjemmeautomatisering, kalender, alt via MCP |
| Nye backends | Enhver LLM eller lokal model, der passer til `stream_turn` interface |
| Persona skabeloner | ME.md skabeloner for forskellige sprog og personligheder |
| Forskning | Bedre √∏nsker modeller, hukommelsesudtr√¶k, theory-of-mind prompting |
| Dokumentation | Tutorials, walkthroughs, overs√¶ttelser |

Se [CONTRIBUTING.md](./CONTRIBUTING.md) for udviklingsops√¶tning, kodestil og PR-retningslinjer.

Hvis du er usikker p√•, hvor du skal starte, [√•bn et issue](https://github.com/lifemate-ai/familiar-ai/issues) ‚Äî glad for at pege dig i den rigtige retning.

---

## Licens

[MIT](./LICENSE)

[‚Üí English README](../README.md)
